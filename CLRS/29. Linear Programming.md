# 29. Linear Programming

## ðŸ“‹ Chapter Overview

- 29.1 Linear programming formulations & algorithms: model components, standard form, and high-level simplex vs interior-point methods.

---

## 29.1 Linear Programming Formulations & Algorithms

Linear programming (LP) is one of the most important and widely used optimization models in computer science, operations research, economics, engineering, and many other fields.

### Components of a Linear Program

A linear program consists of:

- **Decision variables** â€” values we are solving for
- **Objective function** â€” a linear expression to maximize or minimize
- **Constraints** â€” linear inequalities or equalities that must be satisfied

### Standard Form of a Linear Program

$$
\begin{align}
\text{Maximize (or Minimize)} \quad & z = c_1x_1 + c_2x_2 + \cdots + c_nx_n \\
\text{subject to} \quad & a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n \leq b_1 \\
& a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n \leq b_2 \\
& \vdots \\
& a_{m1}x_1 + a_{m2}x_2 + \cdots + a_{mn}x_n \leq b_m \\
& x_1, x_2, \ldots, x_n \geq 0
\end{align}
$$

> **Note:** There are several equivalent forms: minimization, equality constraints, and free variables are also used depending on the problem.

### Geometric Interpretation

- **Feasible region** â€” the set of all points satisfying all constraints; forms a convex polyhedron
- **Optimal solution** â€” exists at a vertex (corner point) of this polyhedron (if it exists)
- **Why this matters** â€” the simplex method exploits this property by moving along vertices

### Main Algorithms Covered

#### Simplex Method (Dantzig, 1947)

- Moves from vertex to vertex along edges of the polyhedron
- Very efficient in practice (almost always polynomial time)
- Worst-case exponential time (known pathological examples exist)
- Most widely implemented method

#### Interior-Point Methods (Karmarkar 1984 & others)

- Move through the interior of the polyhedron rather than along edges
- **Guaranteed** polynomial-time complexity
- Often competitive with or faster than simplex on very large problems
- Theoretically elegant but can have larger constant factors

> **CLRS Focus:** This chapter emphasizes the simplex method (especially the revised simplex method) and briefly covers interior-point methods.

### How the Simplex Method Works

1. **Convert to standard form** â€” add slack variables to convert inequalities into equalities
2. **Find initial solution** â€” find a basic feasible solution (BFS); each corresponds to a vertex
3. **Pivot loop:**
   - Choose an **entering variable** â€” one that will improve the objective
   - Choose a **leaving variable** â€” the one that hits zero first
   - Perform a **pivot** â€” update the basis to reflect the new vertex
4. **Termination:**
   - **Optimal** â€” when no improving direction exists
   - **Unbounded** â€” when an unbounded ray is detected
   - **Infeasible** â€” when no feasible solution exists

### Simplex Algorithm in Pseudocode

**Tableau Form Representation:**
SIMPLEX(A, b, c) // maximize c^T x s.t. A x â‰¤ b, x â‰¥ 0
convert to standard form: A x + I s = b, x â‰¥ 0, s â‰¥ 0
initialize basic variables = slack variables s
build initial tableau

        while true:
                if all reduced costs â‰¥ 0:
                        return current basic solution (optimal)

                if some reduced cost < 0 but no positive entry in that column:
                        return "unbounded"

                choose entering variable j   // most negative reduced cost
                choose leaving variable i    // min-ratio test: smallest bÌ„_i / Ä_ij > 0
                pivot on element Ä_ij        // update tableau

````

#### Implementation: Tableau-Based Simplex Solver

```ts
// âš ï¸ Educational implementationâ€”NOT production-ready
// â€¢ Solves maximization problems in inequality form
// â€¢ No degeneracy handling or numerical stability guards

function simpleSimplex(
  c: number[], // objective coefficients [c1, c2, ..., cn]
  A: number[][], // constraint matrix m Ã— n
  b: number[], // right-hand side m
): { solution: number[]; value: number } | string {
  const m = A.length; // number of constraints
  const n = c.length; // number of variables

  // Build tableau: m+1 rows, n+m+1 columns
  // [ A | I | b ]
  // [ -cáµ€ | 0 | 0 ]
  const tableau: number[][] = Array(m + 1)
    .fill(0)
    .map(() => Array(n + m + 1).fill(0));

  // Fill A part
  for (let i = 0; i < m; i++) {
    for (let j = 0; j < n; j++) {
      tableau[i][j] = A[i][j];
    }
    tableau[i][n + i] = 1; // slack variables
    tableau[i][n + m] = b[i]; // right-hand side
  }

  // Objective row (negated for maximization)
  for (let j = 0; j < n; j++) {
    tableau[m][j] = -c[j];
  }

  // Simplex loop
  while (true) {
    // Find entering column (most negative in objective row)
    let enter = -1;
    let minReduced = 0;
    for (let j = 0; j < n + m; j++) {
      if (tableau[m][j] < minReduced) {
        minReduced = tableau[m][j];
        enter = j;
      }
    }
    if (enter === -1) {
      // Optimal
      const value = -tableau[m][n + m];
      const solution = new Array(n).fill(0);
      for (let i = 0; i < m; i++) {
        for (let j = 0; j < n; j++) {
          if (tableau[i][j] === 1 && tableau[i][n + m] > 0) {
            solution[j] = tableau[i][n + m];
          }
        }
      }
      return { solution, value };
    }

    // Find leaving row (min ratio test)
    let leave = -1;
    let minRatio = Infinity;
    for (let i = 0; i < m; i++) {
      if (tableau[i][enter] > 0) {
        const ratio = tableau[i][n + m] / tableau[i][enter];
        if (ratio < minRatio) {
          minRatio = ratio;
          leave = i;
        }
      }
    }

    if (leave === -1) {
      return "Problem is unbounded";
    }

    // Pivot on tableau[leave][enter]
    const pivot = tableau[leave][enter];
    for (let j = 0; j <= n + m; j++) {
      tableau[leave][j] /= pivot;
    }

    for (let i = 0; i <= m; i++) {
      if (i !== leave) {
        const factor = tableau[i][enter];
        for (let j = 0; j <= n + m; j++) {
          tableau[i][j] -= factor * tableau[leave][j];
        }
      }
    }
  }
}

// EXAMPLE: Maximize 3xâ‚ + 5xâ‚‚
// Constraints:
//   xâ‚        â‰¤ 4      (production limit 1)
//   2xâ‚‚       â‰¤ 12     (production limit 2)
//   3xâ‚ + 2xâ‚‚ â‰¤ 18     (resource constraint)
//   xâ‚, xâ‚‚    â‰¥ 0      (non-negativity)

const c = [3, 5];
const A = [
  [1, 0],
  [0, 2],
  [3, 2],
];
const b = [4, 12, 18];

const result = simpleSimplex(c, A, b);

if (typeof result === "string") {
  console.log("Status:", result);
} else {
  console.log("âœ… Optimal solution found");
  console.log("  Objective value: z =", result.value);        // Expected: 36
  console.log("  Solution: xâ‚ =", result.solution[0], ", xâ‚‚ =", result.solution[1]);
  // Expected: xâ‚ = 2, xâ‚‚ = 6, z = 36
}
````

### Key Insights

| Aspect                | Details                                                                                   |
| --------------------- | ----------------------------------------------------------------------------------------- |
| **Complexity**        | Polynomial-time solvable (Khachiyan 1979, Karmarkar 1984); simplex worst-case exponential |
| **Practical Use**     | Simplex method dominates in practice â€” extremely efficient on most real instances         |
| **Alternatives**      | Interior-point methods are polynomial-time and often faster on very large problems        |
| **Applications**      | Resource allocation, scheduling, network flow, diet problems, portfolio optimization      |
| **Edge Cases**        | Degeneracy and cycling can occur; solved via Bland's rule or perturbation                 |
| **Industry Standard** | Most solvers use revised simplex (implicit inverse) or interior-point methods             |

### ðŸ“ Key Takeaway

**Linear programming** optimizes a linear objective function subject to linear constraints. The **simplex method** moves from vertex to vertex along the feasible polyhedronâ€”simple, widely implemented, and extremely efficient in practice despite theoretical worst-case exponential complexity. **Interior-point methods** offer guaranteed polynomial-time performance. LP's true power lies in its ability to model countless real-world optimization problems exactly or approximately.

---

## 29.2 Formulating Problems as Linear Programs

This section teaches one of the most powerful skills in optimization: **problem reformulation**. We learn how to express seemingly combinatorial or discrete problems as linear programs that can be solved efficiently.

### The Modeling Challenge

Given a complex optimization problem, the goal is to transform it into standard LP form: a linear objective + linear constraints + non-negativity bounds.

Once in LP form, we can solve it using powerful polynomial-time algorithms (simplex, interior-point methods) or industry-standard solvers (Gurobi, CPLEX, GLPK, etc.).

### Common Problem Types

| Problem Type            | LP Application                                            |
| ----------------------- | --------------------------------------------------------- |
| **Network & Flow**      | Maximum flow, min-cost flow, shortest paths               |
| **Matching & Covering** | Maximum bipartite matching, vertex cover, set cover       |
| **Assignment**          | Transportation, task assignment, scheduling               |
| **Resource Allocation** | Diet problem, production planning, portfolio optimization |
| **Discrete Variants**   | Knapsack (fractional version), covering problems          |

### Important Examples Covered

1. **Shortest paths** â€” Single-source shortest paths with non-negative weights
2. **Maximum flow** â€” Max-flow problem on directed graphs
3. **Minimum-cost flow** â€” Flow with capacity and cost constraints
4. **Maximum bipartite matching** â€” Finding maximum sets of non-overlapping edges
5. **Vertex cover relaxation** â€” Foundation for approximation algorithms
6. **Fractional knapsack** â€” Classical LP variant
7. **Diet problem** â€” Classic textbook example
8. **Transportation & assignment** â€” Optimal routing and resource distribution

### Modeling Patterns & Techniques

When formulating a problem as LP, consider these common patterns:

- **Indicator/decision variables** â€” $x_i \in \{0,1\}$ (relaxed to $0 \leq x_i \leq 1$ for LP)
- **Flow variables** â€” Variables on edges representing flow/movement
- **Capacity constraints** â€” Upper bounds on resources: $\sum a_{ij}x_j \leq b_i$
- **Conservation constraints** â€” Inflow = outflow at nodes
- **Objectives** â€” Maximize profit/flow or minimize cost/distance

### General Formulation Steps

Follow this systematic approach to convert any optimization problem to LP:

```text
FORMULATE-AS-LP(problem)
     1. Define decision variables (what are we choosing?)
         - Continuous variables x_j â‰¥ 0
         - Often 0 â‰¤ x_j â‰¤ 1 for fractional decisions or indicators

     2. Define the objective function
         maximize (or minimize) âˆ‘ c_j x_j

     3. Write all constraints as linear inequalities / equalities
         - Capacity / resource limits: âˆ‘ a_{ij} x_j â‰¤ b_i
         - Flow conservation: inflow = outflow for intermediate nodes
         - Demand / supply requirements
         - Variable bounds: x_j â‰¥ 0, x_j â‰¤ u_j (if upper bounds exist)

     4. If the problem requires integer solutions â†’ solve the LP relaxation first
         (many problems have integer optimal solutions when data is integral)
```

#### Example: Maximum Bipartite Matching via LP Formulation

```ts
// Demonstrates LP formulation without a full solver
// Shows how to structure matching as optimization problem
// Solution uses greedy matching (demonstration; real solver gives exact LP solution)

interface Edge {
  from: number; // left side
  to: number; // right side
}

function maxBipartiteMatchingLP(
  leftNodes: number,
  rightNodes: number,
  edges: Edge[],
): { matchingSize: number; assignment: [number, number][] } {
  // Variables: x_e for each edge e (0 â‰¤ x_e â‰¤ 1)
  // Objective: maximize âˆ‘ x_e
  // Constraints:
  //   For each left node u:  âˆ‘_{e leaving u} x_e  â‰¤ 1
  //   For each right node v: âˆ‘_{e entering v} x_e  â‰¤ 1
  //   x_e â‰¥ 0

  // For simplicity, we simulate solving it with a toy solver
  // In real code you would use GLPK.js, lp-solve, or a proper library

  // Step 1: Build incidence structures
  const leftAdj = Array(leftNodes)
    .fill(0)
    .map(() => [] as number[]);
  const rightAdj = Array(rightNodes)
    .fill(0)
    .map(() => [] as number[]);

  edges.forEach((e, idx) => {
    leftAdj[e.from].push(idx);
    rightAdj[e.to].push(idx);
  });

  // Step 2: Greedy fractional solution (upper bound) â€“ for illustration
  // In practice this is solved exactly by simplex or interior-point
  const x = new Array(edges.length).fill(0);

  // Very naive greedy assignment (not optimal LP solution, just demo)
  let matchingSize = 0;
  const assignedLeft = new Set<number>();
  const assignedRight = new Set<number>();

  for (const e of edges) {
    if (!assignedLeft.has(e.from) && !assignedRight.has(e.to)) {
      x[edges.indexOf(e)] = 1;
      assignedLeft.add(e.from);
      assignedRight.add(e.to);
      matchingSize++;
    }
  }

  // In a real LP solver we would get the true maximum (which equals integer optimum here)

  const assignment: [number, number][] = [];
  edges.forEach((e, i) => {
    if (x[i] > 0.999) {
      // threshold for integer solution
      assignment.push([e.from, e.to]);
    }
  });

  return {
    matchingSize,
    assignment,
  };
}

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// Example usage
// Left: 0,1,2   Right: 0,1,2,3
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

const edges: Edge[] = [
  { from: 0, to: 0 },
  { from: 0, to: 1 },
  { from: 1, to: 1 },
  { from: 1, to: 2 },
  { from: 2, to: 2 },
  { from: 2, to: 3 },
];

const result = maxBipartiteMatchingLP(3, 4, edges);

console.log("Maximum matching size (LP relaxation):", result.matchingSize);
console.log("Assignment (left â†’ right):", result.assignment);
// Expected size = 3 (maximum possible)
```

### Implementation Considerations

| Aspect                     | Details                                                                                                 |
| -------------------------- | ------------------------------------------------------------------------------------------------------- |
| **Integrality**            | Many combinatorial problems have integral polyhedraâ€”LP relaxation gives integer solutions automatically |
| **When integrality fails** | Use LP relaxation + rounding as a common approximation technique (vertex cover, set cover, etc.)        |
| **First step**             | Formulating as LP is often the starting point for proving approximation ratios                          |
| **Duality connection**     | Use duality to prove optimality and approximation guarantees                                            |
| **Modern solvers**         | Gurobi, CPLEX, GLPK, CBC, HiGHS can solve very large LPs efficiently                                    |
| **Key skill**              | Knowing how to introduce the right variables and constraints is crucial                                 |

### ðŸ“ Key Takeaway

**Reformulation is powerful.** Many seemingly combinatorial or discrete problems can be exactly formulated as linear programs by carefully defining decision variables (often 0â€“1 or flow-like), linear objectives, and linear constraints. Problems like shortest paths, maximum flow, bipartite matching, and vertex cover become tractable. Once in LP form, powerful polynomial-time algorithms solve themâ€”or use LP relaxation + rounding for approximation.

---

## 29.3 Duality

Duality is one of the most elegant and powerful concepts in linear programming. Every linear program has an associated dual program with a deep symmetrical relationship.

### Why Duality Matters

- **Certificates of optimality** â€” prove when a solution is optimal
- **Bounds on solutions** â€” generate bounds without solving the original problem
- **Proof techniques** â€” prove min-max theorems and approximation ratios
- **Algorithmic insights** â€” many algorithms exploit dual structure

### The Primal-Dual Relationship

#### Primal Linear Program (Maximization)

$$
\begin{align}
\text{maximize} \quad & \mathbf{c}^T \mathbf{x} \\
\text{subject to} \quad & A \mathbf{x} \leq \mathbf{b} \\
& \mathbf{x} \geq \mathbf{0}
\end{align}
$$

#### Dual Linear Program (Minimization)

$$
\begin{align}
\text{minimize} \quad & \mathbf{b}^T \mathbf{y} \\
\text{subject to} \quad & A^T \mathbf{y} \geq \mathbf{c} \\
& \mathbf{y} \geq \mathbf{0}
\end{align}
$$

### Key Structural Relationships

| Primal                        | Dual                            |
| ----------------------------- | ------------------------------- |
| $m$ constraints               | $m$ variables                   |
| $n$ variables                 | $n$ constraints                 |
| Objective coeffs $\mathbf{c}$ | RHS values $\mathbf{b}$         |
| RHS values $\mathbf{b}$       | Objective coeffs $\mathbf{c}$   |
| Maximization                  | Minimization                    |
| $A\mathbf{x} \leq \mathbf{b}$ | $A^T\mathbf{y} \geq \mathbf{c}$ |

### Duality Theorems

#### Weak Duality

For **any** feasible primal solution $\mathbf{x}$ and **any** feasible dual solution $\mathbf{y}$:

$$\mathbf{c}^T\mathbf{x} \leq \mathbf{b}^T\mathbf{y}$$

**Interpretation:** The dual objective always provides an **upper bound** on the primal objective (and vice versa).

#### Strong Duality

If the primal has an optimal solution with value $z^*$, then the dual also has an optimal solution with **exactly the same value**:

$$\text{At optimality:} \quad \mathbf{c}^T\mathbf{x}^* = \mathbf{b}^T\mathbf{y}^* = z^*$$

**Power:** This equality provides certificates of optimality and tight bounds.

### Complementary Slackness Condition

This is the key optimality test for primal and dual solutions.

**Definition:** If $\mathbf{x}^*$ and $\mathbf{y}^*$ are optimal for primal and dual respectively, then:

- **Primal constraint $i$ has slack** (not tight) $\Rightarrow$ **Dual variable** $y_i^* = 0$
- **Dual constraint $j$ has slack** (not tight) $\Rightarrow$ **Primal variable** $x_j^* = 0$

This condition **completely characterizes optimality**: if both conditions hold and both solutions are feasible, they are guaranteed to be optimal.

### Verifying Optimality Using Complementary Slackness

**Verification Procedure:**
TO CHECK OPTIMALITY:
âœ“ Verify both x and y are feasible (satisfy all constraints)
âœ“ For each primal constraint i:
if slack_i > 0 then verify y_i = 0
âœ“ For each dual constraint j:
if slack_j > 0 then verify x_j = 0
âœ“ If all conditions hold â†’ solutions are optimal!

````

#### Example: Converting Primal LP to Dual & Checking Optimality

```ts
interface LinearProgram {
  maximize: boolean; // true = max, false = min
  c: number[]; // objective coefficients
  A: number[][]; // constraint matrix (m rows Ã— n cols)
  b: number[]; // right-hand side
  sense: string[]; // "<=" or ">=" or "="
}

/**
 * Convert a primal LP to its dual
 * Assumes primal is in inequality form with â‰¤ constraints and maximization
 */
function getDualLP(primal: LinearProgram): LinearProgram {
  if (!primal.maximize) {
    throw new Error("Only maximization primal supported in this example");
  }

  const m = primal.A.length; // number of primal constraints
  const n = primal.c.length; // number of primal variables

  // Dual: minimize báµ€y s.t. Aáµ€y â‰¥ c, y â‰¥ 0
  return {
    maximize: false,
    c: primal.b.slice(), // dual objective = primal RHS
    A: primal.A[0].map(
      (
        _,
        col, // transpose A
      ) => primal.A.map((row) => row[col]),
    ),
    b: primal.c.slice(), // dual RHS = primal objective coeffs
    sense: Array(n).fill(">="), // dual constraints are â‰¥
  };
}

/**
 * Very simple check for complementary slackness
 * (assumes you already have primal and dual optimal solutions)
 */
function checkComplementarySlackness(
  primal: LinearProgram,
  x: number[],
  dualY: number[],
  primalSlack: number[],
  dualSlack: number[],
): boolean {
  const m = primal.A.length;
  const n = primal.c.length;

  // Primal slack > 0 â‡’ dual variable = 0
  for (let i = 0; i < m; i++) {
    if (primalSlack[i] > 1e-8 && Math.abs(dualY[i]) > 1e-8) {
      console.log(
        `Violation: primal constraint ${i} has slack but dual y_${i} â‰  0`,
      );
      return false;
    }
  }

  // Dual slack > 0 â‡’ primal variable = 0
  for (let j = 0; j < n; j++) {
    if (dualSlack[j] > 1e-8 && Math.abs(x[j]) > 1e-8) {
      console.log(
        `Violation: dual constraint ${j} has slack but primal x_${j} â‰  0`,
      );
      return false;
    }
  }

  return true;
}

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// Example: small primal LP and its dual
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

const primal = {
  maximize: true,
  c: [3, 5],
  A: [
    [1, 0],
    [0, 2],
    [3, 2],
  ],
  b: [4, 12, 18],
  sense: ["<=", "<=", "<="],
};

console.log("Primal LP:");
console.log("max  3xâ‚ + 5xâ‚‚");
console.log("s.t. xâ‚        â‰¤ 4");
console.log("     2xâ‚‚       â‰¤ 12");
console.log("     3xâ‚ + 2xâ‚‚ â‰¤ 18");
console.log("     xâ‚, xâ‚‚ â‰¥ 0");

const dual = getDualLP(primal);

console.log("\nDual LP:");
console.log("min  4yâ‚ + 12yâ‚‚ + 18yâ‚ƒ");
console.log("s.t. yâ‚       + 3yâ‚ƒ â‰¥ 3");
console.log("     2yâ‚‚      + 2yâ‚ƒ â‰¥ 5");
console.log("     yâ‚, yâ‚‚, yâ‚ƒ â‰¥ 0");

// In a real solver you would solve both and check:
//   - optimal value should be the same
//   - complementary slackness should hold
````

### Duality in Practice

| Concept                      | Role                                                                           |
| ---------------------------- | ------------------------------------------------------------------------------ |
| **Weak Duality**             | Always holds; any feasible dual bounds the primal from above                   |
| **Strong Duality**           | For LPs: optimal primal value = optimal dual value (when both exist)           |
| **Complementary Slackness**  | Clean optimality certificateâ€”no need to re-solve                               |
| **Sensitivity Analysis**     | Dual variables reveal shadow prices and marginal values                        |
| **Min-Max Theorems**         | Max-flow min-cut, KÃ¶nig's theorem, von Neumann's theorem are dual consequences |
| **Approximation Algorithms** | LP relaxation + dual bounds prove approximation ratios                         |
| **Proving Optimality**       | Use duality to create certificates without additional computation              |

### ðŸ“ Key Takeaway

**Duality unlocks deep insights.** Every LP has a dual with symmetrical structureâ€”primal constraints â†” dual variables. **Weak duality** ensures any feasible dual bounds the primal. **Strong duality** guarantees optimal primal = optimal dual. **Complementary slackness** provides precise optimality certificates. Together, these enable proving optimality, designing approximation algorithms, establishing min-max theorems, and sensitivity analysis. Duality reveals fundamental structural properties of optimization problems and transforms intractable primal forms into manageable dual forms.
