# 3. Characterizing Running Times

## ğŸ“‹ Chapter Overview

| Section | Topic               | Key Idea                          |
| ------- | ------------------- | --------------------------------- |
| **3.1** | Asymptotic notation | O / Î© / Î˜ intuition and use cases |
| **3.2** | Formal definitions  | Precise math definitions          |
| **3.3** | Common functions    | Standard growth classes           |

---

## 3.1 O-notation, Î©-notation, and Î˜-notation

This section introduces the three most important **asymptotic notations** used to describe the running time (or space) behavior of algorithms as the input size grows large.

### Why do we need asymptotic notation?

When analyzing algorithms, we usually care about:

- How running time grows as input size **n â†’ âˆ**
- Comparing algorithms in a **machine-independent** way
- Ignoring **constant factors** and **lower-order terms**

An exact running time like:

> `7nÂ² + 3n + 12`

is too precise and depends on hardware and implementation details.
Asymptotic notation focuses on the **dominant term** and gives us clean, comparable bounds.

### The three main notations

| Notation | Intuitive meaning           | Reads as  | Formal meaning (intuition) | Best used for                   |
| -------- | --------------------------- | --------- | -------------------------- | ------------------------------- |
| **O**    | upper bound ("at most")     | big-O     | grows no faster than       | worst-case upper bound          |
| **Î©**    | lower bound ("at least")    | big-Omega | grows at least as fast as  | best-case or lower-bound proofs |
| **Î˜**    | tight bound ("exact order") | big-Theta | grows at the same rate     | precise characterization        |

### Intuitive examples

Let **f(n)** be the running time of an algorithm.

| f(n)              | O(?)       | Î©(?)       | Î˜(?)       | Meaning                 |
| ----------------- | ---------- | ---------- | ---------- | ----------------------- |
| `3n + 7`          | O(n)       | Î©(n)       | Î˜(n)       | linear time             |
| `5nÂ² + 100n + 42` | O(nÂ²)      | Î©(nÂ²)      | Î˜(nÂ²)      | quadratic time          |
| `n log n + 50`    | O(n log n) | Î©(n log n) | Î˜(n log n) | efficient sorting       |
| `2â¿ + nÂ³`         | O(2â¿)      | Î©(2â¿)      | Î˜(2â¿)      | exponential (very slow) |
| `nÂ³`              | O(nâ´)      | Î©(nÂ²)      | â€”          | between nÂ² and nâ´       |
| `17`              | O(1)       | Î©(1)       | Î˜(1)       | constant time           |

### Common asymptotic classes (memorize)

- **Î˜(1)** â€“ constant
- **Î˜(log n)** â€“ logarithmic
- **Î˜(n)** â€“ linear
- **Î˜(n log n)** â€“ linearithmic
- **Î˜(nÂ²)** â€“ quadratic
- **Î˜(nÂ³)** â€“ cubic
- **Î˜(2â¿)** â€“ exponential
- **Î˜(n!)** â€“ factorial

### Quick reference (CLRS-style)

| Algorithm              | Worst      | Average    | Best       |
| ---------------------- | ---------- | ---------- | ---------- |
| Insertion sort         | Î˜(nÂ²)      | Î˜(nÂ²)      | Î˜(n)       |
| Merge sort             | Î˜(n log n) | Î˜(n log n) | Î˜(n log n) |
| Heapsort               | Î˜(n log n) | Î˜(n log n) | Î˜(n log n) |
| Quicksort (randomized) | Î˜(nÂ²)      | Î˜(n log n) | Î˜(n log n) |
| Counting sort          | Î˜(n + k)   | Î˜(n + k)   | Î˜(n + k)   |
| Binary search          | Î˜(log n)   | Î˜(log n)   | Î˜(log n)   |

### Growth rate intuition

Ordered from slowest to fastest:

```
1 < log n < n < n log n < nÂ² < nÂ³ < 2â¿ < n! < nâ¿
```

In practice, most usable algorithms lie between **log n** and **nÂ³**.

### When to use each notation

| Goal                  | Use | Example                  |
| --------------------- | --- | ------------------------ |
| Upper bound is enough | O   | runs in O(nÂ²) time       |
| Prove minimum cost    | Î©   | Î©(n log n) lower bound   |
| Exact growth rate     | Î˜   | merge sort is Î˜(n log n) |

> **Î˜ is the strongest and most precise**. Use it whenever possible.

---

## 3.2 Asymptotic notation: formal definitions

Section 3.2 provides the **precise mathematical definitions** of the three core asymptotic notations:

- **O (big-O)** â€“ asymptotic upper bound
- **Î© (big-Omega)** â€“ asymptotic lower bound
- **Î˜ (big-Theta)** â€“ tight (exact-order) bound

All definitions rely on **positive constants** and a **threshold nâ‚€** beyond which the inequalities hold.

### 1. Î˜-notation (tight bound)

#### Definition

Let **f(n)** and **g(n)** be functions from â„• to â„âº.

We say:

> **f(n) = Î˜(g(n))**

if there exist positive constants **câ‚, câ‚‚, and nâ‚€** such that for all **n â‰¥ nâ‚€**:

```
câ‚ Â· g(n) â‰¤ f(n) â‰¤ câ‚‚ Â· g(n)
```

#### Intuitive meaning

- f(n) is _sandwiched_ between two constant multiples of g(n)
- f and g grow at the **same asymptotic rate**

#### Examples

- `7nÂ² + 4n + 12 = Î˜(nÂ²)`
- `3n log n + 100n = Î˜(n log n)`
- `5 = Î˜(1)`

### 2. O-notation (asymptotic upper bound)

#### Definition

We say:

> **f(n) = O(g(n))**

if there exist positive constants **c and nâ‚€** such that for all **n â‰¥ nâ‚€**:

```
f(n) â‰¤ c Â· g(n)
```

#### Intuitive meaning

- f(n) grows **no faster** than g(n)
- Often used for **worst-case** analysis

#### Important

> Every Î˜ bound implies an O bound, but **not** the other way around.

#### Examples

- `nÂ² + 100n = O(nÂ²)` âœ”
- `nÂ² + 100n = O(nÂ³)` âœ” (looser bound)
- `n log n = O(nÂ²)` âœ”
- `2â¿ â‰  O(nÂ³)` âŒ

### 3. Î©-notation (asymptotic lower bound)

#### Definition

We say:

> **f(n) = Î©(g(n))**

if there exist positive constants **c and nâ‚€** such that for all **n â‰¥ nâ‚€**:

```
f(n) â‰¥ c Â· g(n)
```

#### Intuitive meaning

- f(n) grows **at least as fast** as g(n)
- Used for **lower-bound proofs**

#### Examples

- `nÂ² + 100n = Î©(nÂ²)` âœ”
- `nÂ² + 100n = Î©(n)` âœ”
- `n log n = Î©(n)` âœ”
- `nÂ² â‰  Î©(nÂ³)` âŒ

### Summary: formal definitions side by side

| Notation | Mathematical condition            | Meaning     | Constants  | Implies |
| -------- | --------------------------------- | ----------- | ---------- | ------- |
| Î˜(g(n))  | câ‚g(n) â‰¤ f(n) â‰¤ câ‚‚g(n) for n â‰¥ nâ‚€ | exact order | câ‚, câ‚‚ > 0 | O and Î© |
| O(g(n))  | f(n) â‰¤ c g(n) for n â‰¥ nâ‚€          | upper bound | c > 0      | â€”       |
| Î©(g(n))  | f(n) â‰¥ c g(n) for n â‰¥ nâ‚€          | lower bound | c > 0      | â€”       |

### Implication relationships

```
f(n) = Î˜(g(n))  â‡’  f(n) = O(g(n)) and f(n) = Î©(g(n))

f(n) = O(g(n))  â‡  f(n) = Î©(g(n))

f(n) = Î©(g(n))  â‡  f(n) = O(g(n))
```

### Key Takeaways

- **Î˜** is the strongest statement (upper + lower bound)
- **O** is most common in practice (worst-case)
- **Î©** is critical for proving limits
- Constants and nâ‚€ **do not need to be computed explicitly**â€”only their existence matters

---

## 3.3 Standard notations and common functions

Section 3.3 introduces the **most frequently appearing functions** in asymptotic analysis.
Nearly every running time in CLRS belongs to one (or a combination) of these classes.

### 1. Logarithms (very slow growth)

All logarithms are **asymptotically equivalent** up to constant factors.

#### Change of base

```
log_b n = log_k n / log_k b    (b, k > 1)
```

Therefore:

```
logâ‚‚ n = Î˜(logâ‚ƒ n) = Î˜(ln n) = Î˜(logâ‚â‚€ n)
```

In algorithm analysis, we usually write **log n** (base 2 by convention).

#### Common appearances

- Binary search
- Height of balanced binary trees
- Divide-and-conquer algorithms

### 2. Polynomial functions

Form:

```
n^k   (k > 0, constant)
```

Growth order:

```
1 < n < nÂ² < nÂ³ < nâ´ < ...
```

#### Common appearances

- Î˜(n) â€“ array traversal
- Î˜(nÂ²) â€“ insertion, bubble, selection sort
- Î˜(nÂ³) â€“ naive matrix multiplication

### 3. Linearithmic functions â€” n log n

```
n log n
```

This is the **optimal worst-case bound** for comparison-based sorting.

#### Common appearances

- Merge sort
- Heapsort
- Quicksort (average / randomized)
- Many divide-and-conquer algorithms

### 4. Exponential functions

```
a^n   (a > 1)
```

Grow extremely fast â€” practical only for very small _n_.

#### Key comparison

> Any polynomial n^k grows slower than any exponential a^n (a > 1).

#### Common appearances

- Brute-force TSP
- Subset sum (naive)
- Exhaustive search for NP-complete problems

### 5. Factorial function

```
n!
```

Grows faster than exponentials.

#### Stirlingâ€™s approximation

```
n! â‰ˆ âˆš(2Ï€n) Â· (n/e)^n
```

Therefore:

```
n! = Î˜(âˆšn Â· (n/e)^n)
```

#### Common appearances

- Permutations
- Brute-force TSP
- Combinatorial enumeration

### Growth-rate summary (slowest â†’ fastest)

| Order      | Name         | Examples          | Practical?   |
| ---------- | ------------ | ----------------- | ------------ |
| Î˜(1)       | Constant     | Array access      | Yes          |
| Î˜(log n)   | Logarithmic  | Binary search     | Yes          |
| Î˜(n)       | Linear       | Array scan        | Yes          |
| Î˜(n log n) | Linearithmic | Efficient sorting | Yes          |
| Î˜(nÂ²)      | Quadratic    | Insertion sort    | Small n      |
| Î˜(nÂ³)      | Cubic        | Naive matrix mult | Small n      |
| Î˜(2â¿)      | Exponential  | Subset sum        | Very small n |
| Î˜(n!)      | Factorial    | Permutations      | Tiny n       |

### Approximate values for n = 100

| Function | Value          | Order    |
| -------- | -------------- | -------- |
| logâ‚‚ n   | â‰ˆ 6.6          | 10â°      |
| n        | 100            | 10Â²      |
| n log n  | â‰ˆ 664          | 10Â³      |
| nÂ²       | 10,000         | 10â´      |
| nÂ³       | 1,000,000      | 10â¶      |
| 2â¿       | â‰ˆ 1.27 Ã— 10Â³â°  | 10Â³â°     |
| n!       | â‰ˆ 9.33 Ã— 10Â¹âµâ· | enormous |

### Key Takeaways

- log n grows almost like a constant
- n log n is the **sweet spot** for efficient algorithms
- Polynomial â‰ª exponential â‰ª factorial
- Strong intuition about growth rates is essential

> This completes **Chapter 3**, the mathematical foundation for algorithm analysis.
