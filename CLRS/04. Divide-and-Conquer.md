# 4. Divide-and-Conquer

## ğŸ“‹ Chapter Overview

| Section  | Topic                     | Key Idea                              |
| -------- | ------------------------- | ------------------------------------- |
| **4.1**  | Matrix multiplication     | Naive vs divide-and-conquer           |
| **4.2**  | Strassenâ€™s algorithm      | Reduce multiplications from 8 â†’ 7     |
| **4.3**  | Substitution method       | Prove recurrence guesses by induction |
| **4.4**  | Recursion-tree method     | Sum work across recursion levels      |
| **4.5**  | Master method             | Solve $T(n)=aT(n/b)+f(n)$ cases       |
| **4.6â˜…** | Continuous master theorem | Rigorous proof sketch                 |
| **4.7**  | Akraâ€“Bazzi                | General recurrence solver             |

---

## 4.1 Multiplying square matrices

This section serves as the motivating example for the entire divide-and-conquer chapter.
It shows how a seemingly straightforward problem (matrix multiplication) can be solved more efficiently using divide-and-conquer â€” setting the stage for Strassenâ€™s famous improvement in the next section.

### 1. The standard (naive) algorithm for matrix multiplication

Given two **n Ã— n** matrices **A** and **B**, compute the product matrix **C = A Ã— B**, where:

```
C[i][j] = Î£ (A[i][k] Â· B[k][j]) for k = 1 to n
```

**Time complexity of naive method**

Î˜(nÂ³) â€” because we perform **n multiplications** and **nâˆ’1 additions** for each of the **nÂ² entries** â†’ total â‰ˆ nÂ³ operations.

This is the method almost everyone learns first in school.

### 2. Divide-and-conquer approach to matrix multiplication

**Idea:** Divide each **n Ã— n** matrix into four **(n/2) Ã— (n/2)** submatrices.

```
A = [ Aâ‚â‚  Aâ‚â‚‚ ]    B = [ Bâ‚â‚  Bâ‚â‚‚ ]    C = [ Câ‚â‚  Câ‚â‚‚ ]
    [ Aâ‚‚â‚  Aâ‚‚â‚‚ ]        [ Bâ‚‚â‚  Bâ‚‚â‚‚ ]        [ Câ‚‚â‚  Câ‚‚â‚‚ ]
```

Then the product **C = A Ã— B** can be computed using **eight recursive multiplications** of **(n/2) Ã— (n/2)** matrices:

```
Câ‚â‚ = Aâ‚â‚Â·Bâ‚â‚ + Aâ‚â‚‚Â·Bâ‚‚â‚
Câ‚â‚‚ = Aâ‚â‚Â·Bâ‚â‚‚ + Aâ‚â‚‚Â·Bâ‚‚â‚‚
Câ‚‚â‚ = Aâ‚‚â‚Â·Bâ‚â‚ + Aâ‚‚â‚‚Â·Bâ‚‚â‚
Câ‚‚â‚‚ = Aâ‚‚â‚Â·Bâ‚â‚‚ + Aâ‚‚â‚‚Â·Bâ‚‚â‚‚
```

Plus some additions of **(n/2) Ã— (n/2)** matrices (which take **Î˜(nÂ²)** time).

### Pseudocode â€“ Divide-and-Conquer Matrix Multiplication (recursive version)

```
MATRIX-MULTIPLY(A, B, n)              // A, B are nÃ—n matrices
    if n == 1
        return A[1,1] Ã— B[1,1]

    let C be a new nÃ—n matrix

    let m = n / 2

    Partition A and B into four mÃ—m submatrices:
        A11, A12, A21, A22
        B11, B12, B21, B22

    C11 = MATRIX-ADD( MATRIX-MULTIPLY(A11, B11), MATRIX-MULTIPLY(A12, B21) )
    C12 = MATRIX-ADD( MATRIX-MULTIPLY(A11, B12), MATRIX-MULTIPLY(A12, B22) )
    C21 = MATRIX-ADD( MATRIX-MULTIPLY(A21, B11), MATRIX-MULTIPLY(A22, B21) )
    C22 = MATRIX-ADD( MATRIX-MULTIPLY(A21, B12), MATRIX-MULTIPLY(A22, B22) )

    return C
```

### Recurrence relation (running time)

$$
T(n) = 8 T(n/2) + Î˜(nÂ²)
$$

- 8 recursive calls on size **n/2**
- Î˜(nÂ²) time for partitioning + adding the results

This recurrence solves to **Î˜(nÂ³)** â€” exactly the same asymptotic time as the naive algorithm.

â†’ So far, divide-and-conquer gave us **no improvement**!

This is intentional â€” it sets up the dramatic improvement in the next section (**4.2 Strassenâ€™s algorithm**).

### TypeScript â€“ Naive Matrix Multiplication (iterative version)

```ts
function matrixMultiply(A: number[][], B: number[][]): number[][] {
  const n = A.length;
  const C: number[][] = Array(n)
    .fill(0)
    .map(() => Array(n).fill(0));

  for (let i = 0; i < n; i++) {
    for (let j = 0; j < n; j++) {
      for (let k = 0; k < n; k++) {
        C[i][j] += A[i][k] * B[k][j];
      }
    }
  }

  return C;
}
// Example usage (small 2Ã—2 matrices)
const A = [
  [1, 3],
  [7, 5],
];
const B = [
  [6, 8],
  [4, 2],
];

console.log("A Ã— B =");
console.table(matrixMultiply(A, B));
// Result:
// [18, 14]
// [62, 66]
```

### TypeScript â€“ Very basic recursive divide-and-conquer version

_(only for n being power of 2)_

```ts
function matrixMultiplyRecursive(A: number[][], B: number[][]): number[][] {
  const n = A.length;

  if (n === 1) {
    return [[A[0][0] * B[0][0]]];
  }

  const m = n / 2;

  // Extract submatrices (this part is verbose in practice)
  const A11 = A.slice(0, m).map((row) => row.slice(0, m));
  const A12 = A.slice(0, m).map((row) => row.slice(m));
  const A21 = A.slice(m).map((row) => row.slice(0, m));
  const A22 = A.slice(m).map((row) => row.slice(m));

  const B11 = B.slice(0, m).map((row) => row.slice(0, m));
  const B12 = B.slice(0, m).map((row) => row.slice(m));
  const B21 = B.slice(m).map((row) => row.slice(0, m));
  const B22 = B.slice(m).map((row) => row.slice(m));

  // 8 recursive multiplications
  const C11 = matrixAdd(
    matrixMultiplyRecursive(A11, B11),
    matrixMultiplyRecursive(A12, B21),
  );
  const C12 = matrixAdd(
    matrixMultiplyRecursive(A11, B12),
    matrixMultiplyRecursive(A12, B22),
  );
  const C21 = matrixAdd(
    matrixMultiplyRecursive(A21, B11),
    matrixMultiplyRecursive(A22, B21),
  );
  const C22 = matrixAdd(
    matrixMultiplyRecursive(A21, B12),
    matrixMultiplyRecursive(A22, B22),
  );

  // Combine into result matrix (omitted for brevity - you would stitch them back)
  // In real code we would create full nÃ—n and copy submatrices in place

  throw new Error("Full implementation omitted - too verbose for demo");
}

// Helper (addition of two matrices)
function matrixAdd(A: number[][], B: number[][]): number[][] {
  const n = A.length;
  const C: number[][] = Array(n)
    .fill(0)
    .map(() => Array(n).fill(0));

  for (let i = 0; i < n; i++) {
    for (let j = 0; j < n; j++) {
      C[i][j] = A[i][j] + B[i][j];
    }
  }

  return C;
}
```

### Key Takeaway from Section 4.1

- The standard matrix multiplication is **Î˜(nÂ³)**
- A naive divide-and-conquer approach using **8 recursive calls** does not improve the asymptotic complexity â€” still **Î˜(nÂ³)**

This motivates the famous question:

**Can we do better than 8 recursive multiplications?**

â†’ **Answer:** Yes â€” **Strassenâ€™s algorithm** (next section **4.2**) reduces it to **7 recursive multiplications** â†’ asymptotic time **Î˜(n^{logâ‚‚7}) â‰ˆ Î˜(n^{2.807})**

This is the first historical example where divide-and-conquer gave a provably better asymptotic complexity than the classical method.

---

## 4.2 Strassenâ€™s algorithm for matrix multiplication

Strassenâ€™s algorithm (1969) is a classic breakthrough in algorithm design â€” it was the first to show that the standard Î˜(nÂ³) complexity for nÃ—n matrix multiplication is not optimal.

### The big idea

Instead of 8 recursive multiplications of (n/2)Ã—(n/2) matrices (as in the naive divide-and-conquer method from 4.1),
Strassen discovered a way to compute the product using only 7 recursive multiplications
(at the cost of more additions and subtractions).

â†’ This gives a better asymptotic running time.

### Recurrence comparison

**Naive divide-and-conquer (section 4.1):**

$$
T(n) = 8 T(n/2) + Î˜(nÂ²)  â†’  Î˜(nÂ³)
$$

**Strassenâ€™s algorithm:**

$$
T(n) = 7 T(n/2) + Î˜(nÂ²)  â†’  Î˜(n^{logâ‚‚7}) â‰ˆ Î˜(n^{2.8074})
$$

Even though the exponent improvement is small (~0.1926), it becomes very significant for large matrices.

### The 7 magic products (Strassenâ€™s formulas)

Let the input matrices be partitioned as:

```
A = [ Aâ‚â‚  Aâ‚â‚‚ ]      B = [ Bâ‚â‚  Bâ‚â‚‚ ]
    [ Aâ‚‚â‚  Aâ‚‚â‚‚ ]          [ Bâ‚‚â‚  Bâ‚‚â‚‚ ]
```

Strassen computes the following 7 intermediate matrices:

```
Mâ‚ = (Aâ‚â‚ + Aâ‚‚â‚‚)(Bâ‚â‚ + Bâ‚‚â‚‚)
Mâ‚‚ = (Aâ‚‚â‚ + Aâ‚‚â‚‚) Bâ‚â‚
Mâ‚ƒ = Aâ‚â‚ (Bâ‚â‚‚ âˆ’ Bâ‚‚â‚‚)
Mâ‚„ = Aâ‚â‚‚ (Bâ‚‚â‚ âˆ’ Bâ‚â‚)
Mâ‚… = (Aâ‚â‚ + Aâ‚â‚‚) Bâ‚‚â‚‚
Mâ‚† = (Aâ‚‚â‚ âˆ’ Aâ‚â‚)(Bâ‚â‚ + Bâ‚â‚‚)
Mâ‚‡ = (Aâ‚â‚‚ âˆ’ Aâ‚‚â‚‚)(Bâ‚‚â‚ + Bâ‚‚â‚‚)
```

Then the result blocks are:

```
Câ‚â‚ = Mâ‚ + Mâ‚„ âˆ’ Mâ‚… + Mâ‚‡
Câ‚â‚‚ = Mâ‚ƒ + Mâ‚…
Câ‚‚â‚ = Mâ‚‚ + Mâ‚„
Câ‚‚â‚‚ = Mâ‚ âˆ’ Mâ‚‚ + Mâ‚ƒ + Mâ‚†
```

These formulas are clever algebraic identities â€” they are correct but not obvious.

### Pseudocode â€“ STRASSEN (high-level)

```
STRASSEN(A, B, n)                 // nÃ—n matrices, n is power of 2
    if n == 1
        return [[A[1,1] Ã— B[1,1]]]

    Partition A and B into four (n/2)Ã—(n/2) submatrices

    Compute the 7 products recursively:
        Mâ‚ â† STRASSEN(Aâ‚â‚ + Aâ‚‚â‚‚, Bâ‚â‚ + Bâ‚‚â‚‚, n/2)
        Mâ‚‚ â† STRASSEN(Aâ‚‚â‚ + Aâ‚‚â‚‚, Bâ‚â‚,        n/2)
        Mâ‚ƒ â† STRASSEN(Aâ‚â‚,        Bâ‚â‚‚ âˆ’ Bâ‚‚â‚‚, n/2)
        Mâ‚„ â† STRASSEN(Aâ‚â‚‚,        Bâ‚‚â‚ âˆ’ Bâ‚â‚, n/2)
        Mâ‚… â† STRASSEN(Aâ‚â‚ + Aâ‚â‚‚, Bâ‚‚â‚‚,        n/2)
        Mâ‚† â† STRASSEN(Aâ‚‚â‚ âˆ’ Aâ‚â‚, Bâ‚â‚ + Bâ‚â‚‚, n/2)
        Mâ‚‡ â† STRASSEN(Aâ‚â‚‚ âˆ’ Aâ‚‚â‚‚, Bâ‚‚â‚ + Bâ‚‚â‚‚, n/2)

    Compute result submatrices:
        Câ‚â‚ â† Mâ‚ + Mâ‚„ âˆ’ Mâ‚… + Mâ‚‡
        Câ‚â‚‚ â† Mâ‚ƒ + Mâ‚…
        Câ‚‚â‚ â† Mâ‚‚ + Mâ‚„
        Câ‚‚â‚‚ â† Mâ‚ âˆ’ Mâ‚‚ + Mâ‚ƒ + Mâ‚†

    Combine Câ‚â‚, Câ‚â‚‚, Câ‚‚â‚, Câ‚‚â‚‚ into C

    return C
```

### TypeScript â€“ Conceptual illustration of Strassen (simplified, educational)

```ts
// Note: Real implementation needs careful matrix addition/subtraction helpers
// Here we show only the structure (not full working code)

function strassen(A: number[][], B: number[][]): number[][] {
  const n = A.length;

  if (n === 1) {
    return [[A[0][0] * B[0][0]]];
  }

  const m = n / 2;

  // Submatrices extraction would go here (A11, A12, A21, A22, etc.)
  // For clarity we skip verbose slicing

  // The 7 recursive multiplications
  const M1 = strassen(add(A11, A22), add(B11, B22));
  const M2 = strassen(add(A21, A22), B11);
  const M3 = strassen(A11, subtract(B12, B22));
  const M4 = strassen(A12, subtract(B21, B11));
  const M5 = strassen(add(A11, A12), B22);
  const M6 = strassen(subtract(A21, A11), add(B11, B12));
  const M7 = strassen(subtract(A12, A22), add(B21, B22));

  // Result submatrices
  const C11 = add(subtract(add(M1, M4), M5), M7);
  const C12 = add(M3, M5);
  const C21 = add(M2, M4);
  const C22 = add(subtract(add(M1, M3), M2), M6);

  // Combine into full nÃ—n matrix (code omitted)
  return combine(C11, C12, C21, C22);
}

// Helper functions (very simplified signatures)
function add(A: number[][], B: number[][]): number[][] {
  /* ... */ return [];
}
function subtract(A: number[][], B: number[][]): number[][] {
  /* ... */ return [];
}
function combine(
  C11: number[][],
  C12: number[][],
  C21: number[][],
  C22: number[][],
): number[][] {
  // stitch submatrices back together
  return [];
}
```

Real implementations are much more involved (in-place operations, avoiding many copies, handling odd sizes, etc.).

### Summary â€“ What you should remember

- Strassenâ€™s algorithm reduces the number of recursive multiplications from 8 â†’ 7
- This changes the exponent from 3.0 â†’ logâ‚‚7 â‰ˆ 2.807
- It is the first sub-cubic algorithm for matrix multiplication
- Practical constant factors are high â†’ for small/medium matrices, standard Î˜(nÂ³) is often faster
- Theoretically very important â€” opened the door to a long line of research on faster matrix multiplication

Later results (Coppersmithâ€“Winograd, subsequent improvements) reached exponents around ~2.37 (as of early 2020s), but Strassen remains the most famous and conceptually simplest improvement.

---

## 4.3 The substitution method for solving recurrences

The substitution method is a very powerful and rigorous technique for solving recurrences.

The idea is simple:

- Guess the form of the solution
- Use mathematical induction to prove that your guess is correct

This method is especially useful when you already have a strong intuition about the answer (often from recursion-tree intuition or experience).

### Basic steps of the substitution method

- Guess the solution form (usually with an unknown constant, e.g. O(n log n), Î˜(nÂ²), etc.)
- Substitute the guess into the recurrence
- Use induction to show it holds
- Handle base cases and choose constants carefully
- (Optional but important) Deal with floors/ceilings when needed

### Example 1: Merge sort recurrence (very common case)

**Recurrence:**

$$
T(n) = 2T(n/2) + n
T(1) = Î˜(1)
$$

**Guess:**

$$
T(n) = O(n lg n)
$$

### Proof by induction (upper bound â€“ big-O style)

**Inductive hypothesis:**

Assume $T(m) â‰¤ c m lg m$ for all m < n (where c > 0 is a constant we will choose later)

**Inductive step (for n > 1):**

```
T(n) = 2T(n/2) + n
     â‰¤ 2(c Â· (n/2) lg(n/2)) + n          (by inductive hypothesis)
     = c n (lg n âˆ’ 1) + n
     = c n lg n âˆ’ c n + n
     = c n lg n + n(1 âˆ’ c)
```

If we choose c â‰¥ 1, then:

```
1 âˆ’ c â‰¤ 0  â†’  n(1 âˆ’ c) â‰¤ 0
```

Therefore:

```
T(n) â‰¤ c n lg n
```

**Conclusion:**

```
T(n) = O(n lg n)
```

We can do a very similar proof for Î©(n lg n) â†’ overall:

```
T(n) = Î˜(n lg n)
```

### Example 2: Strassenâ€™s recurrence

**Recurrence:**

$$
T(n) = 7T(n/2) + Î˜(nÂ²)
$$

$$
T(1) = Î˜(1)
$$

**Guess:**

$$
T(n) = O(n^{\log_2 7}) \quad \text{where } \log_2 7 \approx 2.807
$$

### Proof sketch (upper bound)

Assume:

$$
T(m) â‰¤ c m^{logâ‚‚7}  for all m < n
$$

Then:

```
T(n) = 7T(n/2) + d nÂ²    (for some constant d > 0)
     â‰¤ 7 c (n/2)^{logâ‚‚7} + d nÂ²
     = 7 c n^{logâ‚‚7} / 2^{logâ‚‚7} + d nÂ²
     = c n^{logâ‚‚7} + d nÂ²
```

Since $logâ‚‚7 > 2$, for large n the $n^{logâ‚‚7}$ term dominates nÂ².

We can choose c large enough so that:

$$
c n^{logâ‚‚7} + d nÂ² â‰¤ c n^{logâ‚‚7}
$$

Therefore:

$$
T(n) = O(n^{logâ‚‚7})
$$

### When substitution method is especially useful

- When you want a tight bound (Î˜)
- When the recurrence has floors/ceilings (master method has trouble with them)
- When you need to prove upper and lower bounds separately
- For recurrences that donâ€™t fit the master theorem exactly

### Typical TypeScript-like pseudocode style (for intuition)

```ts
// Conceptual â€“ not real code, just showing the idea of induction

function proveMergeSort(n: number, c: number = 2): boolean {
  // Base case
  if (n <= 1) {
    return true; // T(1) = constant â‰¤ c * 1 * log(1) (we define log(1)=0 or adjust)
  }

  // Inductive step simulation
  const left = proveMergeSort(Math.floor(n / 2), c);
  const right = proveMergeSort(Math.ceil(n / 2), c);

  if (!left || !right) return false;

  // Check: T(n) â‰¤ c n log n
  const predicted = c * n * Math.log2(n);
  const actualApproximation = 2 * (c * (n / 2) * Math.log2(n / 2)) + n;

  return actualApproximation <= predicted;
}
```

### Summary â€“ Substitution method in one sentence

You guess the solution form â†’ then use mathematical induction to prove that your guess satisfies the recurrence for some choice of constants.

This is one of the most fundamental proof techniques in algorithm analysis â€” you'll see it used repeatedly throughout the book.

---

## 4.4 The recursion-tree method for solving recurrences

The recursion-tree method is one of the most intuitive ways to understand and solve recurrence relations.
It is particularly helpful when you want to see where the cost is coming from at each level of recursion.

### Core Idea

We draw the recursion as a tree:

- Each node represents one recursive call
- The cost of that call (excluding recursive subcalls) is written at the node
- We sum the costs level by level, then sum across all levels

This method gives excellent intuition and often leads to a correct asymptotic bound very quickly.

### Classic Example 1: Merge Sort

**Recurrence:**

$ T(n) = 2T(n/2) + n $

$ T(1) = Î˜(1) $

```
      n                  â† level 0: divide + combine cost
    /   \
 n/2     n/2              â† level 1: each subproblem contributes n/2
 /   \   /   \
n/4  n/4 n/4  n/4          â† level 2
 ...  ... ...  ...
```

- Number of levels â‰ˆ logâ‚‚ n
- Cost per level = n (every level has total work n)
- Total cost = n Ã— (number of levels) â‰ˆ n Ã— logâ‚‚ n

**Conclusion:**

$
T(n) = Î˜(n log n)
$

### Example 2: Strassenâ€™s matrix multiplication

**Recurrence:**

$
T(n) = 7T(n/2) + Î˜(nÂ²)
$

```
        cnÂ²                        â† level 0
   /   /   |   \   \   \   \
cnÂ²/4 cnÂ²/4 ... cnÂ²/4 ... cnÂ²/4     â† level 1 (7 subproblems)
   ...
```

- Work at level $ i = 7â± Ã— (c nÂ² / 4â±) = c nÂ² (7/4)â±$
- Number of levels â‰ˆ $logâ‚‚ n$
- This is a geometric series with ratio $r = 7/4 > 1$
- The last level dominates (largest cost)

**Conclusion:**

$ T(n) = Î˜(n^{logâ‚‚7}) â‰ˆ Î˜(n^{2.807}) $

### Example 3: A case where the root dominates

**Recurrence:**

$ T(n) = 3T(n/4) + nÂ² $

```
      nÂ²
   /    |    \
nÂ²/16 nÂ²/16 nÂ²/16
  ...    ...    ...
```

- Cost at level $i = 3â± Ã— (nÂ² / 16â±) = nÂ² (3/16)â±$
- Ratio $r = 3/16 < 1$ â†’ geometric series decreasing
- Root level dominates

**Total cost:**

$ T(n) = Î˜(nÂ²) $

### General strategy for recursion-tree analysis

1. Draw the first few levels of recursion
2. Identify the pattern of cost per level
3. Determine whether the cost:
   - increases down the tree (last level dominates)
   - decreases (root dominates)
   - stays roughly the same (all levels similar â†’ multiply by height)

4. Count the height of the tree (usually Î˜(log n))
5. Sum the series (often geometric)

### Summary Table â€“ Typical Cases

| Recurrence            | Cost per level pattern | Dominating part  | Asymptotic bound |
| --------------------- | ---------------------- | ---------------- | ---------------- |
| $T(n) = 2T(n/2) + n$  | constant (n)           | all levels equal | $Î˜(n log n)$     |
| $T(n) = 7T(n/2) + nÂ²$ | increasing (7/4)       | leaves           | $Î˜(n^{logâ‚‚7})$   |
| $T(n) = 3T(n/4) + nÂ²$ | decreasing (3/16)      | root             | $Î˜(nÂ²)$          |
| $T(n) = T(n/2) + 1$   | decreasing             | root             | $Î˜(log n)$       |
| $T(n) = 4T(n/2) + nÂ²$ | constant (nÂ²)          | all levels equal | $Î˜(nÂ² log n)$    |

### When to use the recursion-tree method?

- You want intuition about where the cost is concentrated
- The recurrence is relatively simple (constant number of subproblems)
- You want to prepare for or confirm the result of the master theorem
- Floors and ceilings are not too messy (otherwise â†’ substitution method)

This method is very powerful for building intuition â€” many students find it the easiest way to understand divide-and-conquer costs.

---

## 4.5 The master method for solving recurrences

The master method (also called the master theorem) is the fastest and most convenient way to solve a very large class of divide-and-conquer recurrences of the form:

$ T(n) = a T(n/b) + f(n) $

where:

- **a â‰¥ 1** â€” number of subproblems
- **b > 1** â€” size-reduction factor (each subproblem is size n/b)
- **f(n)** â€” cost of divide and combine steps (outside the recursion)

The master theorem gives us the asymptotic bound in three cases â€” almost mechanically.

### Three cases of the Master Theorem

Let $n^{log_b a}$ be the total work done at the leaves (when the recursion reaches the base case).

Compare $f(n)$ with $n^{log_b a}$:

| Case | Condition on f(n)                            | Asymptotic bound of T(n)     | Intuition                                       |
| ---- | -------------------------------------------- | ---------------------------- | ----------------------------------------------- |
| 1    | $f(n) = O(n^{log_b a âˆ’ Îµ})$ for some $Îµ > 0$ | $Î˜(n^{log_b a})$             | Leaf level dominates (recursion is heaviest)    |
| 2    | $f(n) = Î˜(n^{log_b a} log^k n)$ for $k â‰¥ 0$  | $Î˜(n^{log_b a} log^{k+1} n)$ | All levels cost roughly the same                |
| 3    | $f(n) = Î©(n^{log_b a + Îµ})$ for some $Îµ > 0$ | $Î˜(f(n))$                    | Root level dominates (combine step is heaviest) |

### Important extra condition for Case 3

**Regularity condition:**

There must exist constants **c > 0** and **nâ‚€ > 0** such that:

$a f(n/b) â‰¤ c f(n)$ for all $n â‰¥ nâ‚€$

This condition usually holds when **f(n)** is polynomial.

### Most important examples (memorize these!)

| Recurrence                 | a   | b   | log_b a         | f(n)    | Case | Solution       |
| -------------------------- | --- | --- | --------------- | ------- | ---- | -------------- |
| $T(n) = 2T(n/2) + n$       | 2   | 2   | 1               | n       | 2    | $Î˜(n log n)$   |
| $T(n) = 7T(n/2) + nÂ²$      | 7   | 2   | $logâ‚‚7 â‰ˆ 2.807$ | nÂ²      | 1    | $Î˜(n^{logâ‚‚7})$ |
| $T(n) = 3T(n/4) + nÂ²$      | 3   | 4   | $logâ‚„3 â‰ˆ 0.792$ | nÂ²      | 3    | $Î˜(nÂ²)$        |
| $T(n) = 4T(n/2) + nÂ²$      | 4   | 2   | 2               | nÂ²      | 2    | $Î˜(nÂ² log n)$  |
| $T(n) = 2T(n/2) + n log n$ | 2   | 2   | 1               | n log n | 2    | $Î˜(n logÂ² n)$  |
| $T(n) = T(n/2) + 1$        | 1   | 2   | 0               | 1       | 2    | $Î˜(log n)$     |
| $T(n) = 9T(n/3) + n$       | 9   | 3   | 2               | n       | 1    | $Î˜(nÂ²)$        |

### Pseudocode style summary (how to apply the master method)

```text
APPLY-MASTER-THEOREM (T(n) = a T(n/b) + f(n))

1. Compute critical exponent: p = log_b a
2. Compare f(n) with n^p:

   Case 1: if f(n) = O(n^{p âˆ’ Îµ}) for some Îµ > 0
      â†’ T(n) = Î˜(n^p)

   Case 2: if f(n) = Î˜(n^p log^k n) for k â‰¥ 0
      â†’ T(n) = Î˜(n^p log^{k+1} n)

   Case 3: if f(n) = Î©(n^{p + Îµ}) for some Îµ > 0
      AND a f(n/b) â‰¤ c f(n) for some c < 1 and large n
      â†’ T(n) = Î˜(f(n))
```

### Summary â€“ When you can use the master theorem

Use it when:

- Recurrence is of form $T(n) = a T(n/b) + f(n)$
- **a** and **b** are constants
- **n/b** is exact (or we ignore floors/ceilings for asymptotic result)

It covers most divide-and-conquer algorithms you will encounter in practice.
This is probably the most frequently used tool for solving recurrences in the rest of the book.

---

## 4.6 Proof of the continuous master theorem

Section 4.6 is a â˜… starred section (advanced/optional) that provides a rigorous proof of the continuous version of the master theorem.

### Continuous form of the recurrence

This version deals with recurrences of the form:

$$
T(x) = a T(x/b) + f(x)    for real-valued x â‰¥ xâ‚€ > 0
$$

(with suitable base case for small x)

This continuous formulation is mathematically cleaner and avoids many of the floor/ceiling complications that appear in the discrete case (n/b, n integer).

The proof presented in CLRS 4th edition is quite technical â€” it uses integration and change of variables to transform the recurrence into a form that can be solved explicitly.

### Main result (continuous master theorem â€“ simplified statement)

Assume **a â‰¥ 1**, **b > 1** are constants, and **f(x)** is positive and â€œreasonably smoothâ€.

Let:

$
p = log_b a   (the critical exponent)
$

Then:

- **If** $`f(x) = O(x^{p âˆ’ Îµ})`$ for some Îµ > 0
  â†’ $`T(x) = Î˜(x^p)`$

- **If** $`f(x) = Î˜(x^p log^k x)`$ for some k â‰¥ 0
  â†’ $`T(x) = Î˜(x^p log^{k+1} x)`$

- **If** $`f(x) = Î©(x^{p + Îµ})`$ for some Îµ > 0
  and there exists $`c < 1`$ such that $`a f(x/b) â‰¤ c f(x)`$ for large x
  â†’ $`T(x) = Î˜(f(x))`$

This is almost identical to the discrete master theorem (Section 4.5), except it works directly on real variables and is easier to prove rigorously.

### Key idea of the proof (high-level overview)

The proof transforms the functional equation using a change of variables:

$
Let x = b^t   â†’   t = log_b x
$

Define a new function:

$S(t) = T(b^t)$

Then the recurrence becomes:

$ S(t) = a S(t âˆ’ 1) + g(t) $
where $g(t) = f(b^t)$

This is now a linear nonhomogeneous recurrence in `t`, which is much easier to solve.

The general solution has the form:

```
S(t) = homogeneous solution + particular solution
```

- The **homogeneous part** produces the $`x^{log_b a}`$ term (leaf contribution).
- The **particular solution** depends on the growth of $`g(t) = f(b^t)`$.

After solving for `S(t)`, we substitute back:

$T(x) = S(log_b x)$

This leads directly to the same three cases as the master theorem.

### Why this section is useful (even if you skip the full proof)

- It shows that the master theorem is not magic â€” it follows from solid mathematics.
- It explains why the same three cases appear in both continuous and discrete versions.
- It clarifies why a logarithmic factor appears exactly when `f(x) â‰ˆ x^{log_b a}` (a resonance phenomenon).
- It handles floors and ceilings implicitly â€” the continuous version reflects the â€œidealâ€ behavior.

### Practical takeaway for most students

For the vast majority of algorithm courses and real analysis:

- You do **not** need to reproduce the full proof of the continuous master theorem.
- You **do** need to know how to apply the discrete master theorem from Section 4.5.
- Knowing that a rigorous continuous proof exists gives confidence in the theorem.

### Pseudocode-style summary of the transformation idea

```
Given: T(x) = a T(x/b) + f(x)

1. Set x = b^t      â†’   t = log_b x
2. Define S(t) = T(b^t)
3. Then:
   S(t) = a S(t âˆ’ 1) + g(t),   where g(t) = f(b^t)
4. Solve this linear recurrence for S(t)
5. Substitute back: T(x) = S(log_b x)
```

### Summary â€“ What to remember about Section 4.6

- Section 4.6 gives a clean mathematical proof using a change of variables to a continuous domain.
- It confirms that the three master-theorem cases are mathematically natural.
- Most students never need the full proof after their first algorithms course.
- The key practical skill remains applying the discrete master theorem quickly and correctly.

If you're preparing for exams or interviews â†’ focus on Section 4.5.
If you're interested in theory and mathematical rigor â†’ Section 4.6 is worth studying carefully.

---

## 4.7 Akraâ€“Bazzi recurrences

Section 4.7 presents the **Akraâ€“Bazzi theorem** â€” a very powerful generalization of the Master Theorem.
It can solve many recurrences that the standard Master Theorem cannot handle.

### The form of recurrences covered by Akraâ€“Bazzi

$$
T(x) = g(x) + \sum_{i=1}^{k} a_i \cdot T(b_i x + h_i(x)) \qquad \text{for } x \ge x_0
$$

Where:

- $k \ge 1$ (number of recursive terms)
- $a_i > 0$ (positive constants â€“ weights of subproblems)
- $0 < b_i < 1$ (fractional size reduction factors)
- $g(x)$ is the non-recursive cost (combine/divide cost)
- $h_i(x)$ are â€œsmallâ€ perturbation functions (usually $|h_i(x)| = O(x / \log^2 x)$ or similar)

In most practical cases we see:

$$
T(n) = g(n) + \sum a_i T(b_i n)
$$

(ignoring floors/ceilings and small perturbations)

### The Akraâ€“Bazzi theorem (simplified statement)

There exists a **unique real number** (p) such that:

$$
\sum_{i=1}^{k} a_i b_i^p = 1
$$

Then the asymptotic solution is:

$$
T(x) = \Theta\left(x^p \left(1 + \int_{1}^{x} \frac{g(u)}{u^{p+1}} , du \right)\right)
$$

This integral expression is the most important (and beautiful) part of the theorem.

### Most common practical cases

| $g(x)$ form                    | Integral result                           | Asymptotic result $T(x)$       | Comparison with Master Theorem |
| ------------------------------ | ----------------------------------------- | ------------------------------ | ------------------------------ |
| $$x^c$$                        | $\approx x^{c-p} / (c-p))$ if $$c \ne p$$ | $\Theta(x^p)$                  | Case 1 or 3                    |
| $$x^p$$                        | $$\approx \log x$$                        | $$\Theta(x^p \log x)$$         | Case 2 $(k=0)$                 |
| $$x^p \log^k x$$ $$(k \ge 1)$$ | $$\approx (\log x)^{k+1} / (k+1)$$        | $$\Theta(x^p (\log x)^{k+1})$$ | Case 2 generalized             |
| $$x^p \log \log x$$            | $$\approx \log \log x$$                   | $$\Theta(x^p \log \log x)$$    | â€”                              |

### Step-by-step how to apply Akraâ€“Bazzi

1. Write the recurrence in the form

   $$
   T(n) = g(n) + \sum a_i T(b_i n)
   $$

2. Solve for (p) in the equation

   $$
   \sum a_i \cdot b_i^p = 1
   $$

   (usually numerically or by guessing simple values like 1, 2, $\log_2 3$, etc.)

3. Compute the integral

   $$
   \int_{1}^{n} \frac{g(u)}{u^{p+1}} , du
   $$

4. The dominant term determines the final (\Theta) bound

---

### Important examples

### Example 1

$
T(n) = T(n/3) + T(2n/3) + n
$

- $a_1 = 1, b_1 = 1/3, a_2 = 1, b_2 = 2/3, g(n) = n$
- Solve: $(1/3)^p + (2/3)^p = 1 \Rightarrow p = 1$
- Integral: $\int n / u^{2} , du = \int u^{-1} , du \approx \log n$

$
T(n) = \Theta(n \log n)
$

---

### Example 2

$
T(n) = 2T(n/4) + T(n/2) + n^2
$

- Equation: $2(1/4)^p + (1/2)^p = 1$
- Try $p = 2$: $6/16 < 1$
- Try $p = 1$: equality holds
- Integral: $\int n^2 / u^2 , du \approx \int du = n$

$
T(n) = \Theta(n^2)
$

---

### Example 3 (hard for standard Master)

$
T(n) = T(\sqrt{n}) + n
$

- $n^{-p/2} = 1 \Rightarrow p = 0$
- Integral: $\int n / u , du \approx n \log n$

$
T(n) = \Theta(n \log n)
$

---

### TypeScript â€“ Numerical solver for finding (p) (educational)

```ts
/**
 * Approximate solver for Akraâ€“Bazzi p where Î£ a_i Â· b_i^p = 1
 */
function findAkraBazziP(
  coefficients: { a: number; b: number }[],
  tolerance: number = 1e-8,
  maxIter: number = 100,
): number {
  let low = -10;
  let high = 10;

  for (let i = 0; i < maxIter; i++) {
    const mid = (low + high) / 2;
    let sum = 0;

    for (const { a, b } of coefficients) {
      sum += a * Math.pow(b, mid);
    }

    if (Math.abs(sum - 1) < tolerance) return mid;

    if (sum < 1) high = mid;
    else low = mid;
  }

  return (low + high) / 2;
}
```

---

### When to use Akraâ€“Bazzi instead of Master Theorem

Use Akraâ€“Bazzi when:

- Different subproblem sizes (e.g. (n/3) and (2n/3))
- More than one type of recursive call
- Non-constant number of subproblems
- Recurrences involving roots ((\sqrt{n}), (\sqrt[3]{n}), etc.)

The Akraâ€“Bazzi theorem is one of the most general and powerful tools for solving divide-and-conquer recurrences that appear in real algorithms research.

> **This concludes Chapter 4 â€“ Divide-and-Conquer.**
