# 28. Matrix Operations

## ðŸ“‹ Chapter Overview

| Section | Topic                                | Key Idea                                                                 |
| ------- | ------------------------------------ | ------------------------------------------------------------------------ |
| 28.1    | Solving Systems of Linear Equations  | Gaussian elimination solving $Ax = b$ in $\Theta(n^3)$ time              |
| 28.2    | Inverting Matrices                   | Matrix inversion via Gaussian elimination with augmented identity matrix |
| 28.3    | Symmetric Positive-Definite Matrices | Cholesky decomposition and least-squares approximation                   |

---

## 28.1 Solving systems of linear equations

> Based on CLRS, Chapter 28.1

### Core idea

We want to solve a linear system

$$A x = b,$$

where $A$ is an $n \times n$ matrix and $b$ is a column vector of length $n$.

**Gaussian elimination** transforms the augmented matrix $[A \mid b]$ into (upper) row-echelon form using three elementary row operations:

- Swap two rows.
- Multiply a row by a nonzero scalar.
- Add a multiple of one row to another row.

These operations do not change the solution set. After **forward elimination**, we obtain an upper-triangular system, which we then solve by **back substitution**.

### Complexity

- **Time:** $\Theta(n^3)$ (dominated by the elimination phase).
- **Space:** $\Theta(n^2)$ (to store the matrix).

This is the standard cubic-time algorithm for solving dense linear systems.

### Important concepts

- **Pivoting:** Avoids division by zero and improves numerical stability (partial pivoting is standard in practice).
- **Singular matrix:** If a pivot becomes zero and cannot be fixed by swapping rows, the system may have no solution or infinitely many solutions.
- **LU decomposition:** Gaussian elimination can be interpreted as computing $A = LU$, where $L$ is lower triangular and $U$ is upper triangular (without pivoting). With partial pivoting we get $PA = LU$, where $P$ is a permutation matrix.

CLRS focuses on the basic version without pivoting but mentions the importance of pivoting for numerical stability.

### Pseudocode â€“ Gaussian elimination (no pivoting)

```text
GAUSSIAN-ELIMINATION(A, b)
    // A is nÃ—n matrix, b is column vector of size n
    // Returns solution x or indicates no unique solution

    n â† number of rows in A

    // Make copies of A and b (conceptually form [A | b])
    let A' â† copy of A
    let b' â† copy of b

    // Forward elimination: make A' upper triangular
    for k â† 1 to n - 1
        // Simple version without pivoting
        if A'[k,k] = 0
            error "Zero pivot â€“ pivoting required or singular matrix"

        for i â† k + 1 to n
            factor â† A'[i,k] / A'[k,k]
            for j â† k to n
                A'[i,j] â† A'[i,j] - factor Ã— A'[k,j]
            b'[i] â† b'[i] - factor Ã— b'[k]

    // Back substitution
    let x â† new array of size n
    for i â† n down to 1
        sum â† 0
        for j â† i + 1 to n
            sum â† sum + A'[i,j] Ã— x[j]
        x[i] â† (b'[i] - sum) / A'[i,i]

    return x
```

### TypeScript implementation â€“ Gaussian elimination (no pivoting)

```ts
function gaussianElimination(A: number[][], b: number[]): number[] | null {
  const n = A.length;

  // Create augmented matrix (we'll modify in place for simplicity)
  const aug: number[][] = A.map((row, i) => [...row, b[i]]);

  // Forward elimination
  for (let k = 0; k < n; k++) {
    // No pivoting in this basic version
    if (Math.abs(aug[k][k]) < 1e-10) {
      console.warn("Zero or near-zero pivot detected at position", k);
      return null; // Singular or ill-conditioned (no unique solution)
    }

    for (let i = k + 1; i < n; i++) {
      const factor = aug[i][k] / aug[k][k];

      for (let j = k; j <= n; j++) {
        aug[i][j] -= factor * aug[k][j];
      }
    }
  }

  // Check if system is consistent (back-substitution preparation)
  for (let i = n - 1; i >= 0; i--) {
    if (Math.abs(aug[i][i]) < 1e-10) {
      if (Math.abs(aug[i][n]) > 1e-10) {
        return null; // Inconsistent system
      }
      // Free variable â†’ infinitely many solutions (not handled here)
      return null;
    }
  }

  // Back substitution
  const x: number[] = new Array(n);

  for (let i = n - 1; i >= 0; i--) {
    let sum = 0;
    for (let j = i + 1; j < n; j++) {
      sum += aug[i][j] * x[j];
    }
    x[i] = (aug[i][n] - sum) / aug[i][i];
  }

  return x;
}
```

#### Example usage

```ts
const A = [
  [2, 1, -1],
  [-3, -1, 2],
  [-2, 1, 2],
];

const b = [8, -11, -3];

console.log("Solving Ax = b where:");
console.log("A =", A);
console.log("b =", b);

const solution = gaussianElimination(A, b);

if (solution) {
  console.log("\nSolution x =", solution);
  // Expected: x â‰ˆ [2, 3, -1]
} else {
  console.log("No unique solution.");
}
```

### Important notes

- The basic version shown here does **not** use pivoting â†’ can fail on many practical problems due to zero or very small pivots.
- In real implementations, **partial pivoting** (swap rows to get a large pivot) is almost always used â†’ yields a $PA = LU$ decomposition.
- Time complexity is $\Theta(n^3)$; space complexity is $\Theta(n^2)$ if we store the whole matrix.
- Numerical stability is a major concern in floating-point arithmetic: small pivots amplify rounding errors.
- For large sparse systems, **iterative methods** (Gaussâ€“Seidel, conjugate gradient, etc.) are often preferred over direct methods.

### Key takeaway

Gaussian elimination solves $Ax = b$ in $\Theta(n^3)$ time by transforming $A$ into upper-triangular form using elementary row operations, followed by back substitution. The basic version without pivoting is conceptually simple but numerically unstable; with partial pivoting, it becomes a reliable building block for many linear-algebra algorithms (including LU decomposition) used throughout Chapter 28.

---

## 28.2 Inverting matrices

> Based on CLRS, Chapter 28.2

### Core idea

To compute the inverse of a nonsingular matrix $A$, we want a matrix $A^{-1}$ such that

$$A A^{-1} = I,$$

where $I$ is the $n \times n$ identity matrix.

We can view the inverse as the solution to the matrix equation

$$A X = I,$$

where the columns of $X$ are the solutions of $n$ linear systems:

$$A x_1 = e_1, \; A x_2 = e_2, \; \dots, \; A x_n = e_n,$$

with $e_i$ the standard basis vectors. All $n$ systems can be solved simultaneously by applying Gaussian elimination to the **augmented matrix**

$$[A \mid I].$$

After forward elimination and back substitution, we obtain

$$[I \mid A^{-1}].$$

### Complexity

- **Time:** $\Theta(n^3)$ (same as solving one dense linear system).
- **Space:** $\Theta(n^2)$.

This is asymptotically optimal for dense matrices, since the output $A^{-1}$ itself has $\Theta(n^2)$ entries.

### Pseudocode â€“ matrix inverse via Gaussian elimination (no pivoting)

```text
MATRIX-INVERSE(A)
    // A is nÃ—n nonsingular matrix
    // Returns Aâ»Â¹ or indicates failure

    n â† size of A

    // Create augmented matrix [A | I]
    let aug â† n Ã— (2n) matrix
    for i â† 1 to n
        for j â† 1 to n
            aug[i, j]     â† A[i, j]
            aug[i, n + j] â† (i == j ? 1 : 0)

    // Gaussian elimination (simplified here without pivoting)
    for k â† 1 to n
        if aug[k, k] â‰ˆ 0
            error "Matrix is singular or nearly singular"

        for i â† k + 1 to n
            factor â† aug[i, k] / aug[k, k]
            for j â† k to 2n
                aug[i, j] â† aug[i, j] - factor Ã— aug[k, j]

    // Back substitution for each of the n right-hand sides
    let inverse â† n Ã— n matrix

    for col â† 1 to n
        let rhs â† column (n + col) of aug  // after elimination
        for i â† n downto 1
            sum â† 0
            for j â† i + 1 to n
                sum â† sum + aug[i, j] Ã— inverse[j, col]
            inverse[i, col] â† (rhs[i] - sum) / aug[i, i]

    return inverse
```

### TypeScript implementation â€“ matrix inversion (no pivoting)

```ts
function matrixInverse(A: number[][]): number[][] | null {
  const n = A.length;

  // Create augmented matrix [A | I]
  const aug: number[][] = A.map((row, i) => {
    const newRow = [...row];
    for (let j = 0; j < n; j++) {
      newRow.push(i === j ? 1 : 0);
    }
    return newRow;
  });

  // Forward elimination (without pivoting)
  for (let k = 0; k < n; k++) {
    if (Math.abs(aug[k][k]) < 1e-10) {
      console.warn("Singular or nearly singular matrix at pivot", k);
      return null;
    }

    for (let i = k + 1; i < n; i++) {
      const factor = aug[i][k] / aug[k][k];

      for (let j = k; j < 2 * n; j++) {
        aug[i][j] -= factor * aug[k][j];
      }
    }
  }

  // Back substitution for all n columns
  const inverse: number[][] = Array(n)
    .fill(0)
    .map(() => Array(n).fill(0));

  for (let col = 0; col < n; col++) {
    for (let i = n - 1; i >= 0; i--) {
      let sum = 0;
      for (let j = i + 1; j < n; j++) {
        sum += aug[i][j] * inverse[j][col];
      }
      inverse[i][col] = (aug[i][n + col] - sum) / aug[i][i];
    }
  }

  return inverse;
}
```

#### Example usage

```ts
const A = [
  [1, 2, 3],
  [0, 1, 4],
  [5, 6, 0],
];

console.log("Original matrix A:");
console.table(A);

const inv = matrixInverse(A);

if (inv) {
  console.log("\nComputed inverse Aâ»Â¹:");
  console.table(inv);

  // Optional: verify A Ã— Aâ»Â¹ â‰ˆ I
  const product = multiplyMatrices(A, inv);
  console.log("\nA Ã— Aâ»Â¹ (should be close to identity):");
  console.table(product);
} else {
  console.log("Matrix is singular or nearly singular.");
}

// Helper: matrix multiplication (for verification)
function multiplyMatrices(a: number[][], b: number[][]): number[][] {
  const rowsA = a.length;
  const colsA = a[0].length;
  const colsB = b[0].length;
  const result = Array(rowsA)
    .fill(0)
    .map(() => Array(colsB).fill(0));

  for (let i = 0; i < rowsA; i++) {
    for (let j = 0; j < colsB; j++) {
      for (let k = 0; k < colsA; k++) {
        result[i][j] += a[i][k] * b[k][j];
      }
    }
  }

  return result;
}
```

### Important notes

- The matrix must be **invertible** (nonsingular, $\det(A) \neq 0$).
- If a pivot becomes zero during elimination â†’ the matrix is singular (no inverse exists).
- Explicit matrix inversion is often **numerically unstable**; in practice, it is better to keep an LU (or related) factorization and solve systems via forward/back substitution.
- Time complexity is $\Theta(n^3)$ â€” same as solving a single dense linear system.
- Many numerical libraries (LAPACK, NumPy, MATLAB) prefer LU / Cholesky / QR factorizations over explicit inversion.

### Key takeaway

The inverse of a matrix $A$ can be computed by row-reducing the augmented matrix $[A \mid I]$ to $[I \mid A^{-1}]$ using Gaussian elimination. This takes $\Theta(n^3)$ time but is rarely the best numerical choice; solving systems via factorizations (e.g., LU) is usually more stable and efficient.

---

## 28.3 Symmetric positive-definite matrices and least-squares approximation

> Based on CLRS, Chapter 28.3

This section covers two closely related topics:

- **Symmetric positive-definite (SPD) matrices** â€“ an important class of matrices with excellent numerical and algorithmic properties.
- **Least-squares approximation** â€“ the standard way to solve overdetermined systems of linear equations ($m > n$).

### Symmetric positive-definite matrices

A square matrix $A$ is **symmetric positive-definite (SPD)** if:

- $A$ is symmetric: $A = A^T$.
- $x^T A x > 0$ for all nonzero vectors $x$ (strict positive definiteness; $\ge 0$ gives positive semidefiniteness).

**Key properties of SPD matrices:**

- All eigenvalues are positive.
- All pivots in Gaussian elimination are positive (no pivoting is needed for stability).
- **Cholesky factorization** always exists: $A = L L^T$ where $L$ is lower triangular with positive diagonal entries.
- Cholesky is about **twice as fast** as general LU decomposition (thanks to symmetry).
- Cholesky is numerically very stable for truly SPD matrices.

### Least-squares approximation

Given an overdetermined system $Ax \approx b$ with $m$ equations and $n$ unknowns ($m > n$), we want $x$ that minimizes the squared error

$$\|Ax - b\|_2^2 = (Ax - b)^T (Ax - b).$$

The solution satisfies the **normal equations**

$$A^T A x = A^T b.$$

If $A$ has full column rank ($\text{rank}(A) = n$), then $A^T A$ is symmetric positive-definite, so we can solve the normal equations efficiently using Cholesky factorization.

### Cholesky decomposition (for SPD matrices)

For SPD $A$, Cholesky writes

$$A = L L^T,$$

where $L$ is lower triangular with positive diagonal entries.

Advantages:

- Numerically stable (no pivoting needed).
- About **half** the floating-point operations of LU.
- Requires $\Theta(n^3)$ time and $\Theta(n^2)$ space.

### Pseudocode â€“ Cholesky + least squares

```text
CHOLESKY-DECOMPOSITION(A)
    // A is nÃ—n symmetric positive-definite
    // Returns lower-triangular L such that A = L Láµ€

    for j â† 1 to n
        L[j, j] â† âˆš( A[j, j] - âˆ‘_{k=1}^{j-1} L[j, k]^2 )

        for i â† j + 1 to n
            L[i, j] â† ( A[i, j] - âˆ‘_{k=1}^{j-1} L[i, k] L[j, k] ) / L[j, j]

    return L


SOLVE-LEAST-SQUARES(A, b)
    // A is mÃ—n (m â‰¥ n), full column rank

    Compute ATA â† Aáµ€ A
    Compute ATb â† Aáµ€ b

    Compute Cholesky factorization ATA = L Láµ€

    Solve L y = ATb      // forward substitution
    Solve Láµ€ x = y       // back substitution

    return x
```

### TypeScript implementation â€“ Cholesky + least-squares

```ts
function choleskyDecomposition(A: number[][]): number[][] | null {
  const n = A.length;
  const L: number[][] = Array(n)
    .fill(0)
    .map(() => Array(n).fill(0));

  for (let j = 0; j < n; j++) {
    let sum = 0;
    for (let k = 0; k < j; k++) {
      sum += L[j][k] * L[j][k];
    }

    const diag = A[j][j] - sum;
    if (diag <= 0) {
      console.warn("Matrix is not positive definite");
      return null;
    }

    L[j][j] = Math.sqrt(diag);

    for (let i = j + 1; i < n; i++) {
      let innerSum = 0;
      for (let k = 0; k < j; k++) {
        innerSum += L[i][k] * L[j][k];
      }
      L[i][j] = (A[i][j] - innerSum) / L[j][j];
    }
  }

  return L;
}

function forwardSubstitution(L: number[][], b: number[]): number[] {
  const n = L.length;
  const y: number[] = new Array(n);

  for (let i = 0; i < n; i++) {
    let sum = 0;
    for (let j = 0; j < i; j++) {
      sum += L[i][j] * y[j];
    }
    y[i] = (b[i] - sum) / L[i][i];
  }

  return y;
}

function backSubstitution(L: number[][], y: number[]): number[] {
  const n = L.length;
  const x: number[] = new Array(n);

  for (let i = n - 1; i >= 0; i--) {
    let sum = 0;
    for (let j = i + 1; j < n; j++) {
      sum += L[j][i] * x[j]; // Láµ€ is upper triangular
    }
    x[i] = (y[i] - sum) / L[i][i];
  }

  return x;
}

function solveLeastSquares(A: number[][], b: number[]): number[] | null {
  const m = A.length;
  const n = A[0].length;

  // Compute Aáµ€A and Aáµ€b
  const ATA: number[][] = Array(n)
    .fill(0)
    .map(() => Array(n).fill(0));
  const ATb: number[] = new Array(n).fill(0);

  for (let i = 0; i < m; i++) {
    for (let j = 0; j < n; j++) {
      ATb[j] += A[i][j] * b[i];
      for (let k = 0; k < n; k++) {
        ATA[j][k] += A[i][j] * A[i][k];
      }
    }
  }

  // Cholesky decomposition of Aáµ€A
  const L = choleskyDecomposition(ATA);
  if (!L) return null;

  // Solve L y = Aáµ€b
  const y = forwardSubstitution(L, ATb);

  // Solve Láµ€ x = y
  const x = backSubstitution(L, y);

  return x;
}
```

#### Example: small overdetermined system

```ts
const A = [
  // m = 4 equations, n = 2 unknowns
  [1, 1],
  [1, 2],
  [2, 1],
  [2, 3],
];

const b = [1, 2, 2, 4];

console.log("Solving least-squares: Ax â‰ˆ b");
console.log("A =", A);
console.log("b =", b);

const solution = solveLeastSquares(A, b);

if (solution) {
  console.log("\nLeast-squares solution x =", solution);
  // Expected approximate solution: x â‰ˆ [0.5, 1.5]
} else {
  console.log("Failed â€“ matrix Aáµ€A is not positive definite");
}
```

### Important notes

- Cholesky decomposition requires the matrix to be **symmetric positive-definite**; otherwise it fails (negative or zero diagonal in $L$).
- For least-squares, $A^T A$ is always positive semidefinite; if $A$ has full column rank, then $A^T A$ is positive-definite and Cholesky works.
- Cholesky is about **2Ã— faster** than LU because it exploits symmetry (only one triangle is stored/updated).
- Explicitly forming $A^T A$ can **square the condition number**, causing loss of numerical precision; QR or SVD are often preferred in practice for ill-conditioned problems.
- Least-squares is the foundation of linear regression in statistics and machine learning.

### Key takeaway

Symmetric positive-definite matrices admit a stable and efficient Cholesky factorization $A = L L^T$, which is faster and more stable than general LU. For overdetermined least-squares problems $Ax \approx b$, solving the normal equations $A^T A x = A^T b$ via Cholesky provides an efficient direct method, though in numerically sensitive applications QR or SVD are often preferable.
