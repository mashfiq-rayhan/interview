# 5. Probabilistic Analysis and Randomized Algorithms

## ðŸ“‹ Chapter Overview

| Section  | Topic                           | Key Idea                                          |
| -------- | ------------------------------- | ------------------------------------------------- |
| **5.1**  | Hiring problem                  | Expected hires via indicator variables            |
| **5.2**  | Indicator variables             | Linearity of expectation (no independence needed) |
| **5.3**  | Randomized algorithms           | Las Vegas vs Monte Carlo                          |
| **5.4â˜…** | Advanced probabilistic analysis | Indicators + bounds (union/Markov/Chernoff)       |

---

## 5.1 The Hiring Problem

The hiring problem is a very simple but extremely useful story problem that introduces the power of probabilistic analysis and randomized algorithms.

### Problem Description

- You are the head of a company and need to hire the **best candidate** out of **n applicants**.
- Candidates are interviewed **one by one in random order** (each permutation is equally likely).
- After each interview, you must decide **immediately** whether to hire that person.
- **You cannot go back** to previous candidates.

**Goal:** Hire the best candidate overall (the one with the highest rank).

### Strategy 1: Naive Strategy (always bad)

- Hire the first candidate you see.
- **Probability of success:** 1/n
- Very poor for large n.

### Strategy 2: Optimal-looking but still bad strategy

- Hire the **first candidate who is better than all previous ones**.
- This is called the **"online best-so-far" strategy**.
- Surprisingly, this simple strategy is actually quite good!

### Analysis Using Indicator Random Variables

- Let the candidates be numbered **1 to n** in the order they appear (random permutation).
- Let the **best candidate** be candidate number 1 (the absolute best).
- Define indicator random variables:

  ```
  Xáµ¢ = 1 if candidate i is the best among the first i candidates
  Xáµ¢ = 0 otherwise
  ```

- You hire candidate i exactly when Xáµ¢ = 1.
- Total number of hires:

  $
  X = Xâ‚ + Xâ‚‚ + â€¦ + Xâ‚™
  $

- Expected number of hires:

  $
  E[X] = E[Xâ‚ + Xâ‚‚ + â€¦ + Xâ‚™] = Î£ E[Xáµ¢]
  $

- Probability that Xáµ¢ = 1:

  $
  Pr(Xáµ¢ = 1) = 1/i
  $

- Therefore:

  $
    E[X] = Î£_{i=1}^n (1/i) = Hâ‚™ â‰ˆ ln n + Î³
  $

_(where $Hâ‚™$ is the $n-th$ harmonic number, $Î³ â‰ˆ 0.57721$ is Eulerâ€“Mascheroni constant)_

**Conclusion:** You expect to hire only $â‰ˆ ln n + Î³ â‰ˆ O(log n)$ candidates â€” very few compared to $n!$

#### Probability of Hiring the Absolute Best Candidate

- Let X be the indicator that we hire the best overall candidate.
- We hire the best candidate exactly when the best candidate appears **after position k**, where k is the last position where we hired someone before seeing the best.
- Probability:

  $
  Pr(hire the best) = 1/n Ã— Î£_{k=1}^n (1/k) Ã— (k-1) / (n-1)
  $

- Simplifies to approximately $1/e â‰ˆ 0.3679$ (about 37% chance!).
- This is surprisingly high â€” and it is **asymptotically optimal** (no online strategy can do better than $1/e$ in the limit).

### Pseudocode â€“ Online Hiring Strategy (Best-So-Far)

```
HIRE-ASSISTANT(n)
    best = 0                  // no one hired yet
    for i = 1 to n
        interview candidate i
        if candidate i is better than best
            best = i
            hire candidate i
```

### Key Lessons from 5.1

- Even very simple strategies can have surprisingly good performance when input is random.
- **Indicator random variables** are a powerful tool for calculating expected values.
- The **harmonic number** $Hâ‚™ â‰ˆ ln n$ appears very frequently in probabilistic analysis.
- The famous $1/e â‰ˆ 37%$ success probability is optimal for this online hiring/secretary problem.

> This problem is the classic entry point into **randomized algorithms** and **probabilistic analysis**.

---

## 5.2 Indicator Random Variables

Indicator random variables are one of the most powerful and elegant tools in probabilistic analysis of algorithms. They allow us to easily compute expected values of complicated quantities by breaking them down into simple 0/1 decisions.

### Definition

Let A be some event that may or may not occur during a random process. We define the indicator random variable I_A (or X_A) as:

```
I_A = 1   if event A occurs
I_A = 0   otherwise
```

Very simple â€” just a 0/1 switch.

### The Magic Property (Linearity of Expectation)

- The expected value of an indicator variable is simply the probability:

```
E[I_A] = Pr[A]
```

- Most importantly â€” **linearity of expectation holds even when the variables are dependent**:

```
E[Xâ‚ + Xâ‚‚ + â€¦ + Xâ‚™] = E[Xâ‚] + E[Xâ‚‚] + â€¦ + E[Xâ‚™]
```

This property is extremely strong â€” it works regardless of whether the events are independent or not.

### Classic Example: Hiring Problem (from 5.1)

- Strategy: â€œAlways hire a candidate who is better than all previously interviewed candidatesâ€
- Define:

```
Xáµ¢ = indicator that the i-th candidate is better than all previous iâˆ’1 candidates
```

- Total number of hires:

```
X = Xâ‚ + Xâ‚‚ + â€¦ + Xâ‚™
E[X] = E[Xâ‚] + â€¦ + E[Xâ‚™] = Pr[Xâ‚=1] + â€¦ + Pr[Xâ‚™=1]
```

- Because the order is random, each of the first i candidates is equally likely to be the best among them.

```
Pr[Xáµ¢ = 1] = 1/i
E[X] = Î£_{i=1}^n 1/i = Hâ‚™ â‰ˆ ln n + Î³ â‰ˆ O(log n)
```

- We expect only about **log n hires**!

### Another Classic Example: Expected Number of Left-to-Right Maxima

- Number of times a new maximum appears when scanning a random permutation from left to right.
- Same calculation â†’ expected number is Hâ‚™ â‰ˆ ln n

#### Pseudocode â€“ Counting Left-to-Right Maxima Using Indicators

```
pseudocodeLEFT-TO-RIGHT-MAXIMA(A, n)  // A is random permutation of 1..n
    count â† 0
    current_max â† -âˆž

    for i â† 1 to n
        if A[i] > current_max
            current_max â† A[i]
            count â† count + 1  // indicator X_i = 1 here

    return count
```

- Expected value of count = Î£ Pr(Xáµ¢ = 1) = Hâ‚™

### Key Lessons from Section 5.2

- Indicator variables turn counting problems into simple probability sums.
- Linearity of expectation is extremely powerful â€” no independence required.
- Many seemingly complicated expected-value problems become trivial.
- The harmonic number Hâ‚™ â‰ˆ ln n appears very frequently in randomized algorithms.

> This technique is used repeatedly in the rest of Chapter 5 and throughout the book (especially in randomized algorithms and average-case analysis).

---

## 5.3 Randomized Algorithms

### What is a randomized algorithm?

A randomized algorithm is an algorithm that makes **random choices during its execution**. The randomness can be used to:

- Achieve good expected performance
- Simplify the design
- Avoid bad worst-case inputs
- Produce a correct answer with high probability (even if not always)

We usually analyze two main flavors:

**Las Vegas style**

- Always produces correct output
- Running time is a random variable (we care about expected running time)

**Monte Carlo style**

- Running time is fixed (deterministic)
- Output may be incorrect with small probability (we care about error probability)

### Classic Example 1: Randomized Quicksort (Las Vegas style)

**Standard (bad) quicksort**

- Worst-case time: Î˜(nÂ²) when pivot is always the smallest/largest (sorted or reverse-sorted input).

**Randomized quicksort**

- At each step, choose the pivot **uniformly at random** from the current subarray.
- No input can consistently force bad behavior
- Expected running time becomes Î˜(n log n) (very strong result!)

### Pseudocode â€“ Randomized Quicksort

```
RANDOMIZED-QUICKSORT(A, p, r)
    if p < r
        q â† RANDOMIZED-PARTITION(A, p, r)
        RANDOMIZED-QUICKSORT(A, p, q-1)
        RANDOMIZED-QUICKSORT(A, q+1, r)

RANDOMIZED-PARTITION(A, p, r)
    i â† RANDOM(p, r)                // choose random index
    swap A[i] with A[r]             // put random element at end
    return PARTITION(A, p, r)       // standard partition around A[r]
```

### TypeScript implementation â€“ Randomized Quicksort

```ts
function randomizedQuickSort(
  arr: number[],
  low: number = 0,
  high: number = arr.length - 1,
): void {
  if (low < high) {
    const pivotIndex = randomizedPartition(arr, low, high);
    randomizedQuickSort(arr, low, pivotIndex - 1);
    randomizedQuickSort(arr, pivotIndex + 1, high);
  }
}

function randomizedPartition(arr: number[], low: number, high: number): number {
  const randomIndex = low + Math.floor(Math.random() * (high - low + 1));
  [arr[randomIndex], arr[high]] = [arr[high], arr[randomIndex]];

  const pivot = arr[high];
  let i = low - 1;

  for (let j = low; j < high; j++) {
    if (arr[j] <= pivot) {
      i++;
      [arr[i], arr[j]] = [arr[j], arr[i]];
    }
  }

  [arr[i + 1], arr[high]] = [arr[high], arr[i + 1]];
  return i + 1;
}

const numbers = [64, 34, 25, 12, 22, 11, 90];
console.log("Before:", [...numbers]);
randomizedQuickSort(numbers);
console.log("After: ", numbers);
```

### Classic Example 2: Monte Carlo â€“ Primality Testing (simple version)

A very simple randomized algorithm to test whether a number n is prime:

- Pick a random integer a from 2 to nâˆ’2
- Compute a^{n-1} mod n
- If result â‰  1 â†’ n is composite (definitely)
- If result = 1 â†’ n is probably prime (might be wrong)
- Repeat with different random a to reduce error probability

### Pseudocode â€“ Simple Monte Carlo Primality Test

```
WITNESS(a, n)                   // Is a a witness that n is composite?
    if a^{n-1} â‰¢ 1 (mod n)
        return true              // definitely composite
    return false                 // probably prime

RANDOMIZED-PRIMALITY-TEST(n, k)  // k = number of trials
    if n is 2 or 3
        return true
    if n is even or n < 2
        return false
    for i â† 1 to k
        a â† RANDOM(2, n-2)
        if WITNESS(a, n)
            return "composite"
    return "probably prime"
```

### TypeScript â€“ Simple Fermat-based Monte Carlo primality test

```ts
function modPow(base: bigint, exp: bigint, mod: bigint): bigint {
  let result = 1n;
  base = base % mod;
  while (exp > 0n) {
    if (exp % 2n === 1n) result = (result * base) % mod;
    base = (base * base) % mod;
    exp /= 2n;
  }
  return result;
}

function isProbablyPrime(n: number | bigint, k: number = 10): boolean {
  const bigN = BigInt(n);
  if (bigN <= 1n) return false;
  if (bigN <= 3n) return true;
  if (bigN % 2n === 0n) return false;

  let s = 0n,
    d = bigN - 1n;
  while (d % 2n === 0n) {
    d /= 2n;
    s++;
  }

  for (let i = 0; i < k; i++) {
    const a = 2n + BigInt(Math.floor(Math.random() * Number(bigN - 4n)));
    let x = modPow(a, d, bigN);
    if (x === 1n || x === bigN - 1n) continue;

    let continueOuter = false;
    for (let r = 1n; r < s; r++) {
      x = (x * x) % bigN;
      if (x === bigN - 1n) {
        continueOuter = true;
        break;
      }
    }
    if (continueOuter) continue;
    return false; // definitely composite
  }

  return true; // probably prime
}

console.log(isProbablyPrime(17));
console.log(isProbablyPrime(1000000007));
console.log(isProbablyPrime(9999991));
console.log(isProbablyPrime(99999989));
```

---

### Summary â€“ Key Points About Randomized Algorithms

- **Las Vegas** â€” always correct, runtime is random â†’ good expected time
- **Monte Carlo** â€” fixed time, small error probability â†’ good with high probability
- Randomness can dramatically improve expected performance (e.g., quicksort)
- Randomness can simplify analysis and design
- Widely used in practice: quicksort, primality testing, hashing, load balancing, approximation algorithms, machine learning, etc.

---

## â˜… 5.4 Probabilistic Analysis and Further Uses of Indicator Random Variables

_(Introduction to Algorithms, 4th Edition â€“ CLRS)_

This starred section dives deeper into **probabilistic analysis using indicator random variables** on more advanced examples. It shows how the technique from 5.2 can analyze expected values in hashing, birthday paradox, and streak problems â€” even when events are dependent.

### 1. The Birthday Paradox

**Problem:** How many people k do you need in a room so that the probability that at least two share a birthday is > 50%?

- Assume 365 days, ignore leap years, uniform random birthdays.
- **Surprising answer:** Only k â‰ˆ 23 people!

**Analysis using indicators:**

- Let Xáµ¢â±¼ = indicator that person i and person j share a birthday (i < j)
- There are C(k,2) = k(kâˆ’1)/2 such pairs
- Let X = total number of birthday matches = Î£ Xáµ¢â±¼
- E[X] = Î£ E[Xáµ¢â±¼] = C(k,2) Ã— Pr(Xáµ¢â±¼ = 1)
- Pr(two specific people share birthday) = 1/365
- E[X] = k(kâˆ’1)/(2 Ã— 365)
- Set E[X] â‰ˆ 1 (approx for >50% probability): k(kâˆ’1) â‰ˆ 730 â†’ k â‰ˆ âˆš(2 Ã— 365 Ã— ln 2) â‰ˆ 23

> Note: This is a lower bound; the real probability is higher due to dependencies.

---

### 2. Balls and Bins

**Problem:** Throw m balls independently and uniformly at random into n bins. What is the expected number of empty bins?

- Define Xâ±¼ = 1 if bin j is empty, 0 otherwise
- E[Xâ±¼] = Pr(bin j empty) = (1 âˆ’ 1/n)^m
- Total empty bins: X = Î£ Xâ±¼
- E[X] = n (1 âˆ’ 1/n)^m
- For m = n: E[X] â‰ˆ n/e â‰ˆ 0.368 n
- For m = c n ln n: E[X] â†’ 0 (all bins filled with high probability)

#### Pseudocode â€“ Expected empty bins

```
EXPECTED-EMPTY-BINS(n, m)
    expected = 0
    for j = 1 to n
        prob_empty = (1 - 1/n)^m
        expected = expected + prob_empty
    return expected
```

#### TypeScript â€“ Simulation

```ts
function simulateEmptyBins(
  n: number,
  m: number,
  trials: number = 10000,
): number {
  let totalEmpty = 0;

  for (let t = 0; t < trials; t++) {
    const bins = new Array(n).fill(0);
    for (let i = 0; i < m; i++) {
      const bin = Math.floor(Math.random() * n);
      bins[bin]++;
    }
    totalEmpty += bins.filter((count) => count === 0).length;
  }

  const simulated = totalEmpty / trials;
  const theoretical = n * Math.pow(1 - 1 / n, m);

  console.log(`n = ${n}, m = ${m}`);
  console.log(`  Simulated expected empty: ${simulated.toFixed(4)}`);
  console.log(`  Theoretical:              ${theoretical.toFixed(4)}`);

  return simulated;
}

simulateEmptyBins(100, 100); // ~36.8 empty
simulateEmptyBins(100, 200); // ~13.5 empty
simulateEmptyBins(100, 1000); // ~0.00 empty
```

---

### 3. Streaks in Coin Flips

**Problem:** Flip a fair coin n times. What is the expected length of the longest heads streak?

- Indicators for "streak of exactly k starting at position i":
- Y\_{i,k} = 1 if heads from i to i+kâˆ’1, and tails at iâˆ’1 and i+k (if exist)
- E[Y_{i,k}] = (1/2)^{k+2} (adjust boundaries)
- Sum over possible i and k â†’ expected longest streak â‰ˆ logâ‚‚ n

---

### 4. Hashing with Chaining â€“ Expected Chain Length

**Problem:** Insert n keys into n slots with simple uniform hashing. What is the expected length of the longest chain?

- Indicators: Z\_{i,j} = 1 if key j goes into slot i
- L*i = length of chain in slot i = Î£ Z*{i,j}
- E[L_i] = n/n = 1
- Expected maximum chain length (via union bound + Chernoff) â‰ˆ ln n / ln ln n with high probability

---

### Key Advanced Techniques in 5.4

- **Linearity works even with dependence** â€” just sum probabilities
- **Union bound:** Pr(âˆª A_i) â‰¤ Î£ Pr(A_i)
- **Markovâ€™s inequality:** Pr(X â‰¥ a) â‰¤ E[X]/a for non-negative X
- **Chernoff bounds:** Tighter tail bounds for sums of indicators (later chapters)

---

### Summary Table â€“ Indicator Variable Applications

| Problem                 | Indicator Definition                    | Expected Value Formula |
| ----------------------- | --------------------------------------- | ---------------------- |
| Hiring (best-so-far)    | X_i = 1 if i-th is best among first i   | Î£ 1/i = H_n â‰ˆ ln n     |
| Birthday paradox        | X\_{ij} = 1 if i and j share birthday   | C(k,2)/365 â‰ˆ kÂ²/730    |
| Balls and bins (empty)  | X_j = 1 if bin j empty                  | n (1âˆ’1/n)^m            |
| Longest streak (approx) | Y\_{i,k} = 1 if streak of k starts at i | Î£ (1/2)^{k+2} over i,k |

### Key Takeaway

> Indicator random variables turn complex expected-value problems into simple sums of probabilities. Even with heavy dependencies, linearity of expectation gives exact expected values. Combined with bounds (union, Markov, Chernoff), it becomes one of the most powerful tools in randomized algorithm analysis.
