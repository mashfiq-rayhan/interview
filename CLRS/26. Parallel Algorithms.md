# 26. Parallel Algorithms

## üìã Chapter Overview

| Section | Topic                          | Key Idea                                                                          |
| ------- | ------------------------------ | --------------------------------------------------------------------------------- |
| 26.1    | Fork-Join Parallelism          | Parallel threads using fork/join operations with work-stealing schedulers         |
| 26.2    | Parallel Matrix Multiplication | Divide-and-conquer matrix multiplication achieving $O(n^3)$ work, $O(\lg n)$ span |
| 26.3    | Parallel Merge Sort            | Parallel sorting achieving $O(n \log n)$ work, $O(\log^2 n)$ span                 |

---

## 26.1 The Basics of Fork-Join Parallelism

### üìñ Overview

This section introduces the **fork-join model** ‚Äî one of the simplest and most widely used patterns for expressing parallelism in shared-memory multithreaded programs. Fork-join parallelism is the foundation for many parallel algorithms in the chapter (parallel matrix multiplication, parallel merge sort, etc.).

### üí° Core Idea

The fork-join model is based on two main operations:

| Operation | Description                                                             |
| --------- | ----------------------------------------------------------------------- |
| **fork**  | Creates a new parallel thread of execution (like spawning a child task) |
| **join**  | Waits for a forked thread to finish before continuing                   |

The computation forms a **parallel-control DAG** (directed acyclic graph), where:

- **nodes** = strands (sequential pieces of code)
- **edges** = dependencies (including fork and join edges)

The **work** $W$ of a computation is the total number of primitive operations (same as serial time). The **span** (also called critical-path length or parallel time) is the length of the longest path in the DAG ‚Äî it determines the minimum time possible with infinitely many processors.

**Goal:** Design algorithms with:

- Low work (close to serial algorithm)
- Low span (good parallel speedup)

### üìä Key Performance Measures

Let $P$ be the number of processors.

| Metric                  | Definition                                          |
| ----------------------- | --------------------------------------------------- |
| **Work** $W$            | Total number of operations                          |
| **Span** $S$            | Length of the longest chain of dependent operations |
| **Parallel speedup**    | $\leq \min(W/P, S)$                                 |
| **Ideal parallel time** | $\approx S$ (when $P$ is large)                     |

CLRS uses the fork-join model with **work-stealing schedulers** (as in Cilk, OpenMP, Java ForkJoinPool, etc.):

- **Work bound:** $O(W)$
- **Span bound:** $O(S + \log P)$ with high probability (with randomized work stealing)

### üìù Basic Example ‚Äì Parallel Sum

```text
PARALLEL-SUM(A[1..n])
    if n ‚â§ 1
        return A[1]
    else
        left ‚Üê fork PARALLEL-SUM(A[1..n/2])
        right ‚Üê fork PARALLEL-SUM(A[n/2+1..n])
        join left
        join right
        return left + right
```

**Complexity:**

| Metric   | Value           |
| -------- | --------------- |
| Work $W$ | $\Theta(n)$     |
| Span $S$ | $\Theta(\lg n)$ |

### üìù Pseudocode

**Generic fork-join pattern for divide-and-conquer:**

```text
FORK-JOIN-DIVIDE-AND-CONQUER(problem)
    if problem is small enough
        return SOLVE-DIRECTLY(problem)           // base case (sequential)

    (subproblem1, subproblem2, ...) ‚Üê DIVIDE(problem)

    result1 ‚Üê fork FORK-JOIN-DIVIDE-AND-CONQUER(subproblem1)
    result2 ‚Üê fork FORK-JOIN-DIVIDE-AND-CONQUER(subproblem2)
    ...
    // possibly more forks

    join result1
    join result2
    ...

    return COMBINE(result1, result2, ...)
```

### üíª TypeScript Implementation

> **Note:** Since TypeScript/JavaScript does not have built-in fork-join primitives, we simulate it using `Promise.all` (which behaves similarly to forking several tasks and joining them).

```typescript
// Simulated fork-join parallel sum using Promises
async function parallelSum(arr: number[]): Promise<number> {
  if (arr.length <= 1) {
    return arr[0] ?? 0;
  }

  const mid = Math.floor(arr.length / 2);

  // "fork" two subproblems
  const leftPromise = parallelSum(arr.slice(0, mid));
  const rightPromise = parallelSum(arr.slice(mid));

  // "join" ‚Äî wait for both
  const [leftSum, rightSum] = await Promise.all([leftPromise, rightPromise]);

  return leftSum + rightSum;
}

// ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
// Example usage
// ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

async function main() {
  const largeArray = Array.from({ length: 1000000 }, (_, i) => i + 1);

  console.time("Parallel Sum (simulated)");
  const sum = await parallelSum(largeArray);
  console.timeEnd("Parallel Sum (simulated)");

  console.log("Sum:", sum);
  // Expected: sum from 1 to 1,000,000 = 500000500000
}

main().catch(console.error);
```

### ‚ö†Ô∏è Important Notes

- Fork-join is the simplest and most common pattern for expressing shared-memory parallelism
- It naturally fits divide-and-conquer algorithms (matrix multiplication, merge sort, FFT, etc.)
- **Work** = total number of operations ‚âà serial running time
- **Span** = length of the longest chain of dependencies ‚Üí determines best possible parallel time
- With a good work-stealing scheduler, execution time is $O(W/P + S)$ with high probability
- Java's ForkJoinPool, Cilk, OpenMP tasks, Intel TBB, and many modern parallel libraries use this model
- Fork-join is **not** suitable for all parallel problems ‚Äî some require producer-consumer patterns, pipelines, or data parallelism without recursion
- The recursion depth is often $O(\log n)$ in divide-and-conquer ‚Üí span is often $O(\log n)$ when work is linear

### ‚úÖ Key Takeaway

> **Fork-join parallelism** is a simple and powerful model for expressing parallel computation: **fork** creates parallel subcomputations, **join** waits for them to finish. It naturally fits recursive divide-and-conquer algorithms. The performance is characterized by **work** (total operations) and **span** (critical path length). With a good scheduler (work stealing), fork-join algorithms achieve nearly optimal parallel speedup when the span is small (typically logarithmic). This model forms the foundation for most parallel algorithms presented in Chapter 26.

---

## 26.2 Parallel Matrix Multiplication

### üìñ Overview

This section shows how to parallelize the standard matrix multiplication algorithm using the fork-join parallelism model.

**Goal:** Compute the product $C = A \times B$, where $A$ is $n \times n$, $B$ is $n \times n$, and $C$ is $n \times n$.

### üí° Core Idea

The standard (cubic) matrix multiplication has the following triple loop:

```text
for i ‚Üê 1 to n
    for j ‚Üê 1 to n
        C[i,j] ‚Üê 0
        for k ‚Üê 1 to n
            C[i,j] ‚Üê C[i,j] + A[i,k] √ó B[k,j]
```

This has work $\Theta(n^3)$ and span $\Theta(n)$ ‚Äî the span is linear because of the long k-loop dependency chain.

The parallel version uses **recursive divide-and-conquer** (very similar to Strassen's method structure, but without the clever reduction in work):

1. Divide each input matrix into four $n/2 \times n/2$ submatrices
2. Compute 8 subproducts in parallel (fork them)
3. Combine the results (join and add appropriate subproducts)

### üìä Complexity

| Metric   | Value           | Notes                             |
| -------- | --------------- | --------------------------------- |
| **Work** | $\Theta(n^3)$   | Same as serial                    |
| **Span** | $\Theta(\lg n)$ | Dramatically better parallel time |

### üî¢ Recursive Parallel Matrix Multiplication

Divide each matrix into four quadrants:

```text
A = [ A‚ÇÅ‚ÇÅ  A‚ÇÅ‚ÇÇ ]      B = [ B‚ÇÅ‚ÇÅ  B‚ÇÅ‚ÇÇ ]      C = [ C‚ÇÅ‚ÇÅ  C‚ÇÅ‚ÇÇ ]
    [ A‚ÇÇ‚ÇÅ  A‚ÇÇ‚ÇÇ ]          [ B‚ÇÇ‚ÇÅ  B‚ÇÇ‚ÇÇ ]          [ C‚ÇÇ‚ÇÅ  C‚ÇÇ‚ÇÇ ]
```

Then compute:

| Result   | Formula                       |
| -------- | ----------------------------- |
| $C_{11}$ | $A_{11}B_{11} + A_{12}B_{21}$ |
| $C_{12}$ | $A_{11}B_{12} + A_{12}B_{22}$ |
| $C_{21}$ | $A_{21}B_{11} + A_{22}B_{21}$ |
| $C_{22}$ | $A_{21}B_{12} + A_{22}B_{22}$ |

Each of these **8 multiplications** can be computed in parallel. The additions are also parallelizable, but usually the multiplications dominate.

### üìù Pseudocode

```text
PARALLEL-MATRIX-MULTIPLY(A, B, n)
    if n ‚â§ THRESHOLD
        return SERIAL-MATRIX-MULTIPLY(A, B)      // small matrices ‚Üí sequential

    Partition A and B into four n/2 √ó n/2 submatrices:
        A‚ÇÅ‚ÇÅ, A‚ÇÅ‚ÇÇ, A‚ÇÇ‚ÇÅ, A‚ÇÇ‚ÇÇ
        B‚ÇÅ‚ÇÅ, B‚ÇÅ‚ÇÇ, B‚ÇÇ‚ÇÅ, B‚ÇÇ‚ÇÇ

    // Fork 8 recursive multiplications in parallel
    C‚ÇÅ‚ÇÅ‚ÇÅ ‚Üê fork PARALLEL-MATRIX-MULTIPLY(A‚ÇÅ‚ÇÅ, B‚ÇÅ‚ÇÅ, n/2)
    C‚ÇÅ‚ÇÅ‚ÇÇ ‚Üê fork PARALLEL-MATRIX-MULTIPLY(A‚ÇÅ‚ÇÇ, B‚ÇÇ‚ÇÅ, n/2)
    C‚ÇÅ‚ÇÇ‚ÇÅ ‚Üê fork PARALLEL-MATRIX-MULTIPLY(A‚ÇÅ‚ÇÅ, B‚ÇÅ‚ÇÇ, n/2)
    C‚ÇÅ‚ÇÇ‚ÇÇ ‚Üê fork PARALLEL-MATRIX-MULTIPLY(A‚ÇÅ‚ÇÇ, B‚ÇÇ‚ÇÇ, n/2)
    C‚ÇÇ‚ÇÅ‚ÇÅ ‚Üê fork PARALLEL-MATRIX-MULTIPLY(A‚ÇÇ‚ÇÅ, B‚ÇÅ‚ÇÅ, n/2)
    C‚ÇÇ‚ÇÅ‚ÇÇ ‚Üê fork PARALLEL-MATRIX-MULTIPLY(A‚ÇÇ‚ÇÇ, B‚ÇÇ‚ÇÅ, n/2)
    C‚ÇÇ‚ÇÇ‚ÇÅ ‚Üê fork PARALLEL-MATRIX-MULTIPLY(A‚ÇÇ‚ÇÅ, B‚ÇÅ‚ÇÇ, n/2)
    C‚ÇÇ‚ÇÇ‚ÇÇ ‚Üê fork PARALLEL-MATRIX-MULTIPLY(A‚ÇÇ‚ÇÇ, B‚ÇÇ‚ÇÇ, n/2)

    // Join all subproducts
    join all 8 results

    // Combine (parallel addition can also be done recursively)
    C‚ÇÅ‚ÇÅ ‚Üê C‚ÇÅ‚ÇÅ‚ÇÅ + C‚ÇÅ‚ÇÅ‚ÇÇ
    C‚ÇÅ‚ÇÇ ‚Üê C‚ÇÅ‚ÇÇ‚ÇÅ + C‚ÇÅ‚ÇÇ‚ÇÇ
    C‚ÇÇ‚ÇÅ ‚Üê C‚ÇÇ‚ÇÅ‚ÇÅ + C‚ÇÇ‚ÇÅ‚ÇÇ
    C‚ÇÇ‚ÇÇ ‚Üê C‚ÇÇ‚ÇÇ‚ÇÅ + C‚ÇÇ‚ÇÇ‚ÇÇ

    return C
```

### üíª TypeScript Implementation

```typescript
type Matrix = number[][];

async function parallelMatrixMultiply(
  A: Matrix,
  B: Matrix,
  threshold: number = 64,
): Promise<Matrix> {
  const n = A.length;

  if (n <= threshold) {
    // Base case: standard serial multiplication
    const C: Matrix = Array(n)
      .fill(0)
      .map(() => Array(n).fill(0));
    for (let i = 0; i < n; i++) {
      for (let j = 0; j < n; j++) {
        let sum = 0;
        for (let k = 0; k < n; k++) {
          sum += A[i][k] * B[k][j];
        }
        C[i][j] = sum;
      }
    }
    return C;
  }

  const m = Math.floor(n / 2);

  // Extract submatrices
  const A11 = A.slice(0, m).map((row) => row.slice(0, m));
  const A12 = A.slice(0, m).map((row) => row.slice(m));
  const A21 = A.slice(m).map((row) => row.slice(0, m));
  const A22 = A.slice(m).map((row) => row.slice(m));

  const B11 = B.slice(0, m).map((row) => row.slice(0, m));
  const B12 = B.slice(0, m).map((row) => row.slice(m));
  const B21 = B.slice(m).map((row) => row.slice(0, m));
  const B22 = B.slice(m).map((row) => row.slice(m));

  // Fork 8 parallel recursive calls
  const p1 = parallelMatrixMultiply(A11, B11, threshold);
  const p2 = parallelMatrixMultiply(A12, B21, threshold);
  const p3 = parallelMatrixMultiply(A11, B12, threshold);
  const p4 = parallelMatrixMultiply(A12, B22, threshold);
  const p5 = parallelMatrixMultiply(A21, B11, threshold);
  const p6 = parallelMatrixMultiply(A22, B21, threshold);
  const p7 = parallelMatrixMultiply(A21, B12, threshold);
  const p8 = parallelMatrixMultiply(A22, B22, threshold);

  // Join (await all)
  const [C11p1, C11p2, C12p1, C12p2, C21p1, C21p2, C22p1, C22p2] =
    await Promise.all([p1, p2, p3, p4, p5, p6, p7, p8]);

  // Combine (addition)
  const add = (X: Matrix, Y: Matrix): Matrix =>
    X.map((row, i) => row.map((val, j) => val + Y[i][j]));

  const C11 = add(C11p1, C11p2);
  const C12 = add(C12p1, C12p2);
  const C21 = add(C21p1, C21p2);
  const C22 = add(C22p1, C22p2);

  // Build result matrix
  const C: Matrix = Array(n)
    .fill(0)
    .map(() => Array(n).fill(0));

  for (let i = 0; i < m; i++) {
    for (let j = 0; j < m; j++) {
      C[i][j] = C11[i][j];
      C[i][j + m] = C12[i][j];
      C[i + m][j] = C21[i][j];
      C[i + m][j + m] = C22[i][j];
    }
  }

  return C;
}

// ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
// Example usage
// ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

async function demo() {
  const n = 128;
  const A: Matrix = Array(n)
    .fill(0)
    .map(() =>
      Array(n)
        .fill(0)
        .map(() => Math.floor(Math.random() * 10)),
    );
  const B: Matrix = Array(n)
    .fill(0)
    .map(() =>
      Array(n)
        .fill(0)
        .map(() => Math.floor(Math.random() * 10)),
    );

  console.time("Parallel Matrix Multiply");
  const C = await parallelMatrixMultiply(A, B, 32);
  console.timeEnd("Parallel Matrix Multiply");

  // Just print a small part to verify
  console.log("C[0][0] =", C[0][0]);
}

demo().catch(console.error);
```

### ‚ö†Ô∏è Important Notes

- **Work** = $\Theta(n^3)$ ‚Äî same as the serial algorithm (no asymptotic improvement in total operations)
- **Span** = $\Theta(\lg n)$ ‚Äî excellent parallel scaling (logarithmic critical path)
- The constant factors are higher than Strassen's algorithm due to 8 recursive calls instead of 7
- In practice, the threshold (base case size) is very important for performance
- Real implementations use block-based or tiled approaches + cache optimization + SIMD
- This recursive fork-join version is mainly pedagogical ‚Äî it clearly shows how to get logarithmic span
- For very large matrices, Strassen + fork-join or other hybrid approaches are often used

### ‚úÖ Key Takeaway

> The standard cubic matrix multiplication can be parallelized using recursive divide-and-conquer and fork-join. By dividing each matrix into four quadrants and computing the eight subproducts in parallel, we achieve $\Theta(n^3)$ work (same as sequential) but $\Theta(\lg n)$ span ‚Äî dramatically better parallel time than the $\Theta(n)$ span of a naive parallelization of the triple loop. This is a classic example of how fork-join parallelism can turn a serial $O(n^3)$ algorithm into one with logarithmic parallel time while preserving total work.

---

## 26.3 Parallel Merge Sort

### üìñ Overview

This section shows how to parallelize merge sort using the fork-join model, achieving excellent parallelism while keeping the same work as the sequential version.

### üí° Core Idea

Standard sequential merge sort has:

| Metric   | Sequential Value                   | Naive Parallel                    |
| -------- | ---------------------------------- | --------------------------------- |
| **Work** | $\Theta(n \log n)$                 | $\Theta(n \log n)$                |
| **Span** | $\Theta(\log n)$ (recursion depth) | $\Theta(n)$ (merge is sequential) |

The key insight in CLRS is to **parallelize the merge step itself**.

The algorithm consists of two main parallel components:

1. **Parallel recursive sorts** of the two halves ‚Üí very natural with fork-join
2. **Parallel merge** of two sorted halves ‚Üí this is the non-trivial part

The parallel merge procedure uses a **binary search + fork-join strategy** to split the work evenly across processors.

### üìä Complexity Results

| Metric   | Value              | Notes                                        |
| -------- | ------------------ | -------------------------------------------- |
| **Work** | $\Theta(n \log n)$ | Asymptotically same as sequential merge sort |
| **Span** | $\Theta(\log^2 n)$ | Much better than $\Theta(n)$ naive span      |

This is one of the classic examples showing that we can often achieve **polylogarithmic span** even for algorithms that appear inherently sequential at first glance.

### üîÑ How Parallel Merge Works (High-Level)

Given two sorted arrays `A[p..q]` and `A[q+1..r]`, we want to merge them into a sorted result array in parallel:

1. **Find** the median elements of both subarrays (using binary search in parallel)
2. **Decide** how many elements from each subarray go to the left/right half of the result
3. **Fork** two parallel merge operations on the left halves and right halves
4. **Join** the results

Because each level of recursion halves the problem size and we fork two sub-merges:

- Recursion depth: $O(\log n)$
- Each level takes: $O(\log n)$ time (due to binary search)
- **Total span:** $O(\log n \times \log n) = O(\log^2 n)$

### üìù Pseudocode

**PARALLEL-MERGE-SORT(A, p, r)**

```text
PARALLEL-MERGE-SORT(A, p, r)
    if p ‚â• r
        return                           // base case: 0 or 1 element

    q ‚Üê floor((p + r) / 2)

    // Fork recursive sorts in parallel
    left ‚Üê fork PARALLEL-MERGE-SORT(A, p, q)
    right ‚Üê fork PARALLEL-MERGE-SORT(A, q+1, r)

    // Wait for both sub-sorts
    join left
    join right

    // Now merge the two sorted halves in parallel
    PARALLEL-MERGE(A, p, q, r)
```

**PARALLEL-MERGE(A, p, q, r)**

```text
PARALLEL-MERGE(A, p, q, r)
    // merge sorted A[p..q] and A[q+1..r] into a temporary array, then copy back

    n‚ÇÅ ‚Üê q - p + 1
    n‚ÇÇ ‚Üê r - q

    if n‚ÇÅ + n‚ÇÇ ‚â§ THRESHOLD
        SERIAL-MERGE(A, p, q, r)          // small case ‚Üí sequential merge
        return

    // Find median positions using binary search
    // (detailed steps in book: find how many elements from left and right
    //  go to left half of result)

    k ‚Üê floor((n‚ÇÅ + n‚ÇÇ + 1) / 2)

    // Binary search to find split points i and j such that
    // number of elements ‚â§ median is k

    // Fork two parallel merges
    leftMerge ‚Üê fork PARALLEL-MERGE on left parts
    rightMerge ‚Üê fork PARALLEL-MERGE on right parts

    join leftMerge
    join rightMerge
```

> **Note:** The exact median-finding and splitting logic is a bit more involved ‚Äî CLRS provides the full details.

### üíª TypeScript Implementation

```typescript
type NumberArray = number[];

// Threshold for switching to sequential merge
const MERGE_THRESHOLD = 64;

async function parallelMergeSort(arr: number[]): Promise<number[]> {
  if (arr.length <= 1) return arr;

  const mid = Math.floor(arr.length / 2);

  // Fork recursive sorts
  const leftPromise = parallelMergeSort(arr.slice(0, mid));
  const rightPromise = parallelMergeSort(arr.slice(mid));

  // Join
  const [left, right] = await Promise.all([leftPromise, rightPromise]);

  // Parallel merge
  return await parallelMerge(left, right);
}

async function parallelMerge(
  left: number[],
  right: number[],
): Promise<number[]> {
  if (left.length + right.length <= MERGE_THRESHOLD) {
    // Sequential merge for small arrays
    const result: number[] = [];
    let i = 0,
      j = 0;

    while (i < left.length && j < right.length) {
      if (left[i] <= right[j]) {
        result.push(left[i++]);
      } else {
        result.push(right[j++]);
      }
    }

    return result.concat(left.slice(i)).concat(right.slice(j));
  }

  // Find approximate median using binary search
  const total = left.length + right.length;
  const half = Math.floor(total / 2);

  // Binary search in left to find split point
  let low = Math.max(0, half - right.length);
  let high = Math.min(half, left.length);

  let splitLeft = 0;
  let splitRight = 0;

  while (low <= high) {
    const midLeft = Math.floor((low + high) / 2);
    const midRight = half - midLeft;

    const leftMax = midLeft > 0 ? left[midLeft - 1] : -Infinity;
    const rightMin = midRight < right.length ? right[midRight] : Infinity;

    if (leftMax <= rightMin) {
      splitLeft = midLeft;
      splitRight = midRight;
      low = midLeft + 1;
    } else {
      high = midLeft - 1;
    }
  }

  // Fork two sub-merges
  const leftLeft = left.slice(0, splitLeft);
  const rightLeft = right.slice(0, splitRight);
  const leftRight = left.slice(splitLeft);
  const rightRight = right.slice(splitRight);

  const leftMergedPromise = parallelMerge(leftLeft, rightLeft);
  const rightMergedPromise = parallelMerge(leftRight, rightRight);

  const [leftMerged, rightMerged] = await Promise.all([
    leftMergedPromise,
    rightMergedPromise,
  ]);

  return [...leftMerged, ...rightMerged];
}

// ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
// Demo
// ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

async function runDemo() {
  const arr = Array.from({ length: 100000 }, () =>
    Math.floor(Math.random() * 1000000),
  );

  console.time("Parallel Merge Sort");
  const sorted = await parallelMergeSort(arr);
  console.timeEnd("Parallel Merge Sort");

  // Verify (optional)
  console.log("First 10:", sorted.slice(0, 10));
  console.log("Last 10:", sorted.slice(-10));
}

runDemo().catch(console.error);
```

### ‚ö†Ô∏è Important Notes

- **Total work** remains $\Theta(n \log n)$ ‚Äî asymptotically identical to sequential merge sort
- **Span** is $\Theta(\log^2 n)$ ‚Äî much better than the $\Theta(n)$ span you get if you only parallelize the recursive calls
- The parallel merge step uses binary search to balance the work ‚Üí each level costs $O(\log n)$ time
- Recursion depth is $O(\log n)$ ‚Üí overall span = $O(\log n \times \log n) = O(\log^2 n)$
- In practice, the constant factors are higher than sequential merge sort ‚Äî good threshold is important
- Real-world implementations often use block-based or tiled parallel merge for better cache behavior
- This is one of the best-known examples of achieving polylogarithmic span for a fundamental algorithm

### ‚úÖ Key Takeaway

> **Parallel merge sort** uses fork-join to parallelize both the recursive division and the merge step. By recursively splitting the arrays and merging the sorted halves using a parallel merge procedure (which itself uses binary search and fork-join), we achieve $\Theta(n \log n)$ work and $\Theta(\log^2 n)$ span ‚Äî excellent parallelism while preserving the optimal sequential work. This demonstrates that even the merge step ‚Äî which seems inherently sequential ‚Äî can be effectively parallelized to achieve polylogarithmic parallel time.

---

## üìö Chapter Summary

| Algorithm                          | Work               | Span               | Key Technique                |
| ---------------------------------- | ------------------ | ------------------ | ---------------------------- |
| **Fork-Join Parallelism**          | $O(W)$             | $O(S + \log P)$    | Work-stealing schedulers     |
| **Parallel Matrix Multiplication** | $\Theta(n^3)$      | $\Theta(\lg n)$    | Recursive quadrant division  |
| **Parallel Merge Sort**            | $\Theta(n \log n)$ | $\Theta(\log^2 n)$ | Binary search parallel merge |

### Key Concepts

- **Fork-join parallelism** is a fundamental model for shared-memory multithreading that separates computation into parallel threads (fork) and synchronization points (join)
- **Work and span** are the key metrics: work measures total operations, and span measures the longest chain of dependencies
- **Parallel matrix multiplication** achieves $\Theta(n^3)$ work and $\Theta(\lg n)$ span by recursively dividing matrices and computing subproducts in parallel
- **Parallel merge sort** achieves $\Theta(n \log n)$ work and $\Theta(\log^2 n)$ span by parallelizing both recursion and the merge step using binary search
- All algorithms achieve **optimal or near-optimal speedup** when the span is polylogarithmic relative to work
 